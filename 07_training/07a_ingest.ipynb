{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "id": "hiQ6zAoYhyaA",
    "outputId": "0acee878-1207-42c3-9bee-a594acd44365"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
       "  <td>\n",
       "    <a target=\"_blank\" href=\"https://console.cloud.google.com/ai-platform/notebooks/deploy-notebook?name=Writing an efficient ingest Loop&url=https%3A%2F%2Fgithub.com%2FGoogleCloudPlatform%2Fpractical-ml-vision-book%2Fblob%2Fmaster%2F06_preprocessing%2F07a_ingest.ipynb&download_url=https%3A%2F%2Fgithub.com%2FGoogleCloudPlatform%2Fpractical-ml-vision-book%2Fraw%2Fmaster%2F06_preprocessing%2F07a_ingest.ipynb\">\n",
       "    <img src=\"https://raw.githubusercontent.com/GoogleCloudPlatform/practical-ml-vision-book/master/logo-cloud.png\"/> Run in AI Platform Notebook</a>\n",
       "  </td>\n",
       "  </td>\n",
       "  <td>\n",
       "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/GoogleCloudPlatform/practical-ml-vision-book/blob/master/06_preprocessing/07a_ingest.ipynb\">\n",
       "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
       "  </td>\n",
       "  <td>\n",
       "    <a target=\"_blank\" href=\"https://github.com/GoogleCloudPlatform/practical-ml-vision-book/blob/master/06_preprocessing/07a_ingest.ipynb\">\n",
       "    <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
       "  </td>\n",
       "  <td>\n",
       "    <a href=\"https://raw.githubusercontent.com/GoogleCloudPlatform/practical-ml-vision-book/master/06_preprocessing/07a_ingest.ipynb\">\n",
       "    <img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
       "  </td>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown as md\n",
    "\n",
    "### change to reflect your notebook\n",
    "_nb_loc = \"06_preprocessing/07a_ingest.ipynb\"\n",
    "_nb_title = \"Writing an efficient ingest Loop\"\n",
    "\n",
    "### no need to change any of this\n",
    "_nb_safeloc = _nb_loc.replace('/', '%2F')\n",
    "md(\"\"\"\n",
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://console.cloud.google.com/ai-platform/notebooks/deploy-notebook?name={1}&url=https%3A%2F%2Fgithub.com%2FGoogleCloudPlatform%2Fpractical-ml-vision-book%2Fblob%2Fmaster%2F{2}&download_url=https%3A%2F%2Fgithub.com%2FGoogleCloudPlatform%2Fpractical-ml-vision-book%2Fraw%2Fmaster%2F{2}\">\n",
    "    <img src=\"https://raw.githubusercontent.com/GoogleCloudPlatform/practical-ml-vision-book/master/logo-cloud.png\"/> Run in AI Platform Notebook</a>\n",
    "  </td>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/GoogleCloudPlatform/practical-ml-vision-book/blob/master/{0}\">\n",
    "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/GoogleCloudPlatform/practical-ml-vision-book/blob/master/{0}\">\n",
    "    <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://raw.githubusercontent.com/GoogleCloudPlatform/practical-ml-vision-book/master/{0}\">\n",
    "    <img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
    "  </td>\n",
    "</table>\n",
    "\"\"\".format(_nb_loc, _nb_title, _nb_safeloc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a8HQYsAtC0Fv"
   },
   "source": [
    "# Efficient Ingest\n",
    "\n",
    "In this notebook, we improve the training performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5UOm2etrwYCs"
   },
   "source": [
    "## Enable GPU and set up helper functions\n",
    "\n",
    "This notebook and pretty much every other notebook in this repository\n",
    "will run faster if you are using a GPU.\n",
    "On Colab:\n",
    "- Navigate to Editâ†’Notebook Settings\n",
    "- Select GPU from the Hardware Accelerator drop-down\n",
    "\n",
    "On Cloud AI Platform Notebooks:\n",
    "- Navigate to https://console.cloud.google.com/ai-platform/notebooks\n",
    "- Create an instance with a GPU or select your instance and add a GPU\n",
    "\n",
    "Next, we'll confirm that we can connect to the GPU with tensorflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ugGJcxKAwhc2",
    "outputId": "8e946159-46cf-4aba-f53e-622e9ea8adee"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.version.VERSION)\n",
    "device_name = tf.test.gpu_device_name()\n",
    "print(device_name)\n",
    "if device_name != '/device:GPU:0':\n",
    "  raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original code\n",
    "\n",
    "This is the original code, from [../06_preprocessing/06e_colordistortion.ipynb](../06_preprocessing/06e_colordistortion.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import os\n",
    "# Load compressed models from tensorflow_hub\n",
    "os.environ['TFHUB_MODEL_LOAD_FORMAT'] = 'COMPRESSED'    \n",
    "\n",
    "from tensorflow.data.experimental import AUTOTUNE\n",
    "\n",
    "IMG_HEIGHT = 448 # note *twice* what we used to have\n",
    "IMG_WIDTH = 448\n",
    "IMG_CHANNELS = 3\n",
    "CLASS_NAMES = 'daisy dandelion roses sunflowers tulips'.split()\n",
    "\n",
    "def training_plot(metrics, history):\n",
    "  f, ax = plt.subplots(1, len(metrics), figsize=(5*len(metrics), 5))\n",
    "  for idx, metric in enumerate(metrics):\n",
    "    ax[idx].plot(history.history[metric], ls='dashed')\n",
    "    ax[idx].set_xlabel(\"Epochs\")\n",
    "    ax[idx].set_ylabel(metric)\n",
    "    ax[idx].plot(history.history['val_' + metric]);\n",
    "    ax[idx].legend([metric, 'val_' + metric])\n",
    "    \n",
    "class _Preprocessor:    \n",
    "    def __init__(self):\n",
    "        # nothing to initialize\n",
    "        pass\n",
    "    \n",
    "    def read_from_tfr(self, proto):\n",
    "        feature_description = {\n",
    "            'image': tf.io.VarLenFeature(tf.float32),\n",
    "            'shape': tf.io.VarLenFeature(tf.int64),\n",
    "            'label': tf.io.FixedLenFeature([], tf.string, default_value=''),\n",
    "            'label_int': tf.io.FixedLenFeature([], tf.int64, default_value=0),\n",
    "        }\n",
    "        rec = tf.io.parse_single_example(\n",
    "            proto, feature_description\n",
    "        )\n",
    "        shape = tf.sparse.to_dense(rec['shape'])\n",
    "        img = tf.reshape(tf.sparse.to_dense(rec['image']), shape)\n",
    "        label_int = rec['label_int']\n",
    "        return img, label_int\n",
    "    \n",
    "    def read_from_jpegfile(self, filename):\n",
    "        # same code as in 05_create_dataset/jpeg_to_tfrecord.py\n",
    "        img = tf.io.read_file(filename)\n",
    "        img = tf.image.decode_jpeg(img, channels=IMG_CHANNELS)\n",
    "        img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "        return img\n",
    "      \n",
    "    def preprocess(self, img):\n",
    "        return tf.image.resize_with_pad(img, IMG_HEIGHT, IMG_WIDTH)\n",
    "\n",
    "def create_preproc_dataset_old(pattern):\n",
    "    preproc = _Preprocessor()\n",
    "    trainds = tf.data.TFRecordDataset(\n",
    "        [filename for filename in tf.io.gfile.glob(pattern)]\n",
    "    ).map(preproc.read_from_tfr).map(\n",
    "        lambda img, label: (preproc.preprocess(img), label)\n",
    "    )                             \n",
    "    return trainds\n",
    "\n",
    "# note: addition of AUTOTUNE to the map() calls\n",
    "def create_preproc_dataset_interleave(pattern):\n",
    "    preproc = _Preprocessor()\n",
    "    files = [filename for filename in tf.io.gfile.glob(pattern)]\n",
    "    if len(files) > 2:\n",
    "        print(\"Interleaving the reading of {} files.\".format(len(files)))\n",
    "        def _create_half_ds(x):\n",
    "            if x == 0:\n",
    "                half = files[:(len(files)//2)]\n",
    "            else:\n",
    "                half = files[(len(files)//2):]\n",
    "            return tf.data.TFRecordDataset(half)\n",
    "        trainds = tf.data.Dataset.range(2).interleave(\n",
    "            _create_half_ds, num_parallel_calls=AUTOTUNE)\n",
    "    else:\n",
    "        trainds = tf.data.TFRecordDataset(files)\n",
    "    def _preproc_img_label(img, label):\n",
    "        return (preproc.preprocess(img), label)\n",
    "    trainds = (trainds\n",
    "               .map(preproc.read_from_tfr, num_parallel_calls=AUTOTUNE)\n",
    "               .map(_preproc_img_label, num_parallel_calls=AUTOTUNE)\n",
    "              )\n",
    "    return trainds\n",
    "\n",
    "def create_preproc_image(filename):\n",
    "    preproc = _Preprocessor()\n",
    "    img = preproc.read_from_jpegfile(filename)\n",
    "    return preproc.preprocess(img)\n",
    "\n",
    "class RandomColorDistortion(tf.keras.layers.Layer):\n",
    "    def __init__(self, contrast_range=[0.5, 1.5], \n",
    "                 brightness_delta=[-0.2, 0.2], **kwargs):\n",
    "        super(RandomColorDistortion, self).__init__(**kwargs)\n",
    "        self.contrast_range = contrast_range\n",
    "        self.brightness_delta = brightness_delta\n",
    "    \n",
    "    def call(self, images, training=None):\n",
    "        if not training:\n",
    "            return images\n",
    "        \n",
    "        contrast = np.random.uniform(\n",
    "            self.contrast_range[0], self.contrast_range[1])\n",
    "        brightness = np.random.uniform(\n",
    "            self.brightness_delta[0], self.brightness_delta[1])\n",
    "        \n",
    "        images = tf.image.adjust_contrast(images, contrast)\n",
    "        images = tf.image.adjust_brightness(images, brightness)\n",
    "        images = tf.clip_by_value(images, 0, 1)\n",
    "        return images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speeding up the reading of data\n",
    "\n",
    "To try it out, we'll simply read through the data several times and compute some quantity on the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop_through_dataset(ds, nepochs):\n",
    "    lowest_mean = tf.constant(1.)\n",
    "    for epoch in range(nepochs):\n",
    "        thresh = np.random.uniform(0.3, 0.7) # random threshold\n",
    "        count = 0\n",
    "        sumsofar = tf.constant(0.)\n",
    "        for (img, label) in ds:\n",
    "            # mean of channel values > thresh\n",
    "            mean = tf.reduce_mean(tf.where(img > thresh, img, 0))\n",
    "            sumsofar = sumsofar + mean\n",
    "            count = count + 1\n",
    "            if count%10 == 0:\n",
    "                print('.', end='')\n",
    "        mean = sumsofar/count\n",
    "        print(mean)\n",
    "        if mean < lowest_mean:\n",
    "            lowest_mean = mean\n",
    "    return lowest_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATTERN_SUFFIX, NUM_EPOCHS = '-0000*', 3 # small\n",
    "PATTERN_SUFFIX, NUM_EPOCHS = '-*', 20 # full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ds = create_preproc_dataset_old(\n",
    "    'gs://practical-ml-vision-book/flowers_tfr/train' + PATTERN_SUFFIX\n",
    ")\n",
    "loop_through_dataset(ds, NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# with interleave\n",
    "ds = create_preproc_dataset_interleave(\n",
    "    'gs://practical-ml-vision-book/flowers_tfr/train' + PATTERN_SUFFIX\n",
    ")\n",
    "loop_through_dataset(ds, NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alias to the more efficient one\n",
    "def create_preproc_dataset(pattern):\n",
    "    return create_preproc_dataset_interleave(pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# add caching\n",
    "ds = create_preproc_dataset(\n",
    "    'gs://practical-ml-vision-book/flowers_tfr/train' + PATTERN_SUFFIX\n",
    ").cache()\n",
    "loop_through_dataset(ds, NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# add prefetching\n",
    "ds = create_preproc_dataset(\n",
    "    'gs://practical-ml-vision-book/flowers_tfr/train' + PATTERN_SUFFIX\n",
    ").prefetch(AUTOTUNE).cache()\n",
    "loop_through_dataset(ds, NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Add batching.\n",
    "# Note: the loop_through_dataset has to behave properly on a batch ...\n",
    "# (Keras models do)\n",
    "ds = create_preproc_dataset(\n",
    "    'gs://practical-ml-vision-book/flowers_tfr/train' + PATTERN_SUFFIX\n",
    ").prefetch(AUTOTUNE).cache().batch(32, drop_remainder=True)\n",
    "loop_through_dataset(ds, NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(batch_size = 32,\n",
    "                       lrate = 0.001,\n",
    "                       l1 = 0.,\n",
    "                       l2 = 0.,\n",
    "                       num_hidden = 16):\n",
    "    regularizer = tf.keras.regularizers.l1_l2(l1, l2)\n",
    "    \n",
    "    train_dataset = create_preproc_dataset(\n",
    "        'gs://practical-ml-vision-book/flowers_tfr/train' + PATTERN_SUFFIX\n",
    "    ).prefetch(AUTOTUNE).cache().batch(batch_size)\n",
    "    eval_dataset = create_preproc_dataset(\n",
    "        'gs://practical-ml-vision-book/flowers_tfr/valid' + PATTERN_SUFFIX\n",
    "    ).prefetch(AUTOTUNE).cache().batch(batch_size)\n",
    "\n",
    "    layers = [\n",
    "      tf.keras.layers.experimental.preprocessing.RandomCrop(\n",
    "          height=IMG_HEIGHT//2, width=IMG_WIDTH//2,\n",
    "          input_shape=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS),\n",
    "          name='random/center_crop'\n",
    "      ),\n",
    "      tf.keras.layers.experimental.preprocessing.RandomFlip(\n",
    "          mode='horizontal',\n",
    "          name='random_lr_flip/none'\n",
    "      ),\n",
    "      RandomColorDistortion(name='random_contrast_brightness/none'),\n",
    "      hub.KerasLayer(\n",
    "          \"https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4\", \n",
    "          trainable=False,\n",
    "          name='mobilenet_embedding'),\n",
    "      tf.keras.layers.Dense(num_hidden,\n",
    "                            kernel_regularizer=regularizer, \n",
    "                            activation=tf.keras.activations.relu,\n",
    "                            name='dense_hidden'),\n",
    "      tf.keras.layers.Dense(len(CLASS_NAMES), \n",
    "                            kernel_regularizer=regularizer,\n",
    "                            activation='softmax',\n",
    "                            name='flower_prob')\n",
    "    ]\n",
    "\n",
    "    model = tf.keras.Sequential(layers, name='flower_classification')\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lrate),\n",
    "                loss=tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "                    from_logits=False),\n",
    "                metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    history = model.fit(train_dataset, validation_data=eval_dataset, epochs=NUM_EPOCHS)\n",
    "    training_plot(['loss', 'accuracy'], history)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 929
    },
    "id": "jlxxxpeaT6ea",
    "outputId": "ad4f09e8-bc33-4c92-dc47-5fc73ec12f9c"
   },
   "outputs": [],
   "source": [
    "model = train_and_evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model, then load it to make predictions\n",
    "\n",
    "This way, we don't have to have the model in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "shutil.rmtree('export', ignore_errors=True)\n",
    "os.mkdir('export')\n",
    "model.save('export/flowers_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls export/flowers_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!saved_model_cli show --tag_set serve --signature_def serving_default --dir export/flowers_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 682
    },
    "id": "HEh_vh0vvnNd",
    "outputId": "e43ebba2-fe2b-4a53-8b94-32598c021498"
   },
   "outputs": [],
   "source": [
    "# Call model.predict() on a few images in evaluation dataset\n",
    "def plot_predictions(model, pattern):\n",
    "    dataset = create_preproc_dataset(pattern)\n",
    "    f, ax = plt.subplots(3, 5, figsize=(25,15))\n",
    "    for idx, (img, label) in enumerate(dataset.take(15)):\n",
    "        ax[idx//5, idx%5].imshow((img.numpy()));\n",
    "        batch_image = tf.reshape(img, [1, IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS])\n",
    "        batch_pred = model.predict(batch_image)\n",
    "        pred = batch_pred[0]\n",
    "        label = CLASS_NAMES[label.numpy()]\n",
    "        pred_label_index = tf.math.argmax(pred).numpy()\n",
    "        pred_label = CLASS_NAMES[pred_label_index]\n",
    "        prob = pred[pred_label_index]\n",
    "        ax[idx//5, idx%5].set_title('{}: {} ({:.4f})'.format(label, pred_label, prob))\n",
    "\n",
    "serving_model = tf.keras.models.load_model('export/flowers_model')\n",
    "plot_predictions(serving_model, 'gs://practical-ml-vision-book/flowers_tfr/valid-*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## but actually, for prediction, we won't have TensorFlow Records.\n",
    "## this is how we'd predict for individual images\n",
    "## (actually, we'll create a serving function: we'll look at that in a later chapter)\n",
    "filenames = [\n",
    "    'gs://cloud-ml-data/img/flower_photos/dandelion/9818247_e2eac18894.jpg',\n",
    "    'gs://cloud-ml-data/img/flower_photos/dandelion/9853885425_4a82356f1d_m.jpg',\n",
    "    'gs://cloud-ml-data/img/flower_photos/dandelion/98992760_53ed1d26a9.jpg',\n",
    "    'gs://cloud-ml-data/img/flower_photos/dandelion/9939430464_5f5861ebab.jpg',\n",
    "    'gs://cloud-ml-data/img/flower_photos/dandelion/9965757055_ff01b5ee6f_n.jpg'\n",
    "]\n",
    "label = 'dandelion'\n",
    "input_images = [create_preproc_image(f) for f in filenames]\n",
    "f, ax = plt.subplots(1, 5, figsize=(15,15))\n",
    "for idx, img in enumerate(input_images):\n",
    "    ax[idx].imshow((img.numpy()));\n",
    "    batch_image = tf.reshape(img, [1, IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS])\n",
    "    batch_pred = serving_model.predict(batch_image)\n",
    "    pred = batch_pred[0]\n",
    "    pred_label_index = tf.math.argmax(pred).numpy()\n",
    "    pred_label = CLASS_NAMES[pred_label_index]\n",
    "    prob = pred[pred_label_index]\n",
    "    ax[idx].set_title('{}: {} ({:.4f})'.format(label, pred_label, prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Duu8mX3iXANE"
   },
   "source": [
    "## License\n",
    "Copyright 2020 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "5UOm2etrwYCs"
   ],
   "name": "03a_transfer_learning.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "name": "tf2-2-3-gpu.2-3.m59",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-3-gpu.2-3:m59"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
