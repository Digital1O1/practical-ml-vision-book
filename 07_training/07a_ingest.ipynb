{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "id": "hiQ6zAoYhyaA",
    "outputId": "0acee878-1207-42c3-9bee-a594acd44365"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
       "  <td>\n",
       "    <a target=\"_blank\" href=\"https://console.cloud.google.com/ai-platform/notebooks/deploy-notebook?name=Writing an efficient ingest Loop&url=https%3A%2F%2Fgithub.com%2FGoogleCloudPlatform%2Fpractical-ml-vision-book%2Fblob%2Fmaster%2F06_preprocessing%2F07a_ingest.ipynb&download_url=https%3A%2F%2Fgithub.com%2FGoogleCloudPlatform%2Fpractical-ml-vision-book%2Fraw%2Fmaster%2F06_preprocessing%2F07a_ingest.ipynb\">\n",
       "    <img src=\"https://raw.githubusercontent.com/GoogleCloudPlatform/practical-ml-vision-book/master/logo-cloud.png\"/> Run in AI Platform Notebook</a>\n",
       "  </td>\n",
       "  </td>\n",
       "  <td>\n",
       "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/GoogleCloudPlatform/practical-ml-vision-book/blob/master/06_preprocessing/07a_ingest.ipynb\">\n",
       "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
       "  </td>\n",
       "  <td>\n",
       "    <a target=\"_blank\" href=\"https://github.com/GoogleCloudPlatform/practical-ml-vision-book/blob/master/06_preprocessing/07a_ingest.ipynb\">\n",
       "    <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
       "  </td>\n",
       "  <td>\n",
       "    <a href=\"https://raw.githubusercontent.com/GoogleCloudPlatform/practical-ml-vision-book/master/06_preprocessing/07a_ingest.ipynb\">\n",
       "    <img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
       "  </td>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown as md\n",
    "\n",
    "### change to reflect your notebook\n",
    "_nb_loc = \"06_preprocessing/07a_ingest.ipynb\"\n",
    "_nb_title = \"Writing an efficient ingest Loop\"\n",
    "\n",
    "### no need to change any of this\n",
    "_nb_safeloc = _nb_loc.replace('/', '%2F')\n",
    "md(\"\"\"\n",
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://console.cloud.google.com/ai-platform/notebooks/deploy-notebook?name={1}&url=https%3A%2F%2Fgithub.com%2FGoogleCloudPlatform%2Fpractical-ml-vision-book%2Fblob%2Fmaster%2F{2}&download_url=https%3A%2F%2Fgithub.com%2FGoogleCloudPlatform%2Fpractical-ml-vision-book%2Fraw%2Fmaster%2F{2}\">\n",
    "    <img src=\"https://raw.githubusercontent.com/GoogleCloudPlatform/practical-ml-vision-book/master/logo-cloud.png\"/> Run in AI Platform Notebook</a>\n",
    "  </td>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/GoogleCloudPlatform/practical-ml-vision-book/blob/master/{0}\">\n",
    "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/GoogleCloudPlatform/practical-ml-vision-book/blob/master/{0}\">\n",
    "    <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://raw.githubusercontent.com/GoogleCloudPlatform/practical-ml-vision-book/master/{0}\">\n",
    "    <img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
    "  </td>\n",
    "</table>\n",
    "\"\"\".format(_nb_loc, _nb_title, _nb_safeloc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a8HQYsAtC0Fv"
   },
   "source": [
    "# Efficient Ingest\n",
    "\n",
    "In this notebook, we improve the training performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5UOm2etrwYCs"
   },
   "source": [
    "## Enable GPU and set up helper functions\n",
    "\n",
    "This notebook and pretty much every other notebook in this repository\n",
    "will run faster if you are using a GPU.\n",
    "On Colab:\n",
    "- Navigate to Editâ†’Notebook Settings\n",
    "- Select GPU from the Hardware Accelerator drop-down\n",
    "\n",
    "On Cloud AI Platform Notebooks:\n",
    "- Navigate to https://console.cloud.google.com/ai-platform/notebooks\n",
    "- Create an instance with a GPU or select your instance and add a GPU\n",
    "\n",
    "Next, we'll confirm that we can connect to the GPU with tensorflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ugGJcxKAwhc2",
    "outputId": "8e946159-46cf-4aba-f53e-622e9ea8adee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version2.3.1\n",
      "Built with GPU support? Yes!\n",
      "There are 2 GPUs\n",
      "Found GPU at: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print('TensorFlow version' + tf.version.VERSION)\n",
    "print('Built with GPU support? ' + ('Yes!' if tf.test.is_built_with_cuda() else 'Noooo!'))\n",
    "print('There are {} GPUs'.format(len(tf.config.experimental.list_physical_devices(\"GPU\"))))\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original code\n",
    "\n",
    "This is the original code, from [../06_preprocessing/06e_colordistortion.ipynb](../06_preprocessing/06e_colordistortion.ipynb)\n",
    "\n",
    "We have a few variations of creating a preprocessed dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import os\n",
    "# Load compressed models from tensorflow_hub\n",
    "os.environ['TFHUB_MODEL_LOAD_FORMAT'] = 'COMPRESSED'    \n",
    "\n",
    "from tensorflow.data.experimental import AUTOTUNE\n",
    "\n",
    "IMG_HEIGHT = 448 # note *twice* what we used to have\n",
    "IMG_WIDTH = 448\n",
    "IMG_CHANNELS = 3\n",
    "CLASS_NAMES = 'daisy dandelion roses sunflowers tulips'.split()\n",
    "\n",
    "def training_plot(metrics, history):\n",
    "  f, ax = plt.subplots(1, len(metrics), figsize=(5*len(metrics), 5))\n",
    "  for idx, metric in enumerate(metrics):\n",
    "    ax[idx].plot(history.history[metric], ls='dashed')\n",
    "    ax[idx].set_xlabel(\"Epochs\")\n",
    "    ax[idx].set_ylabel(metric)\n",
    "    ax[idx].plot(history.history['val_' + metric]);\n",
    "    ax[idx].legend([metric, 'val_' + metric])\n",
    "    \n",
    "class _Preprocessor:    \n",
    "    def __init__(self):\n",
    "        # nothing to initialize\n",
    "        pass\n",
    "    \n",
    "    def read_from_tfr(self, proto):\n",
    "        feature_description = {\n",
    "            'image': tf.io.VarLenFeature(tf.float32),\n",
    "            'shape': tf.io.VarLenFeature(tf.int64),\n",
    "            'label': tf.io.FixedLenFeature([], tf.string, default_value=''),\n",
    "            'label_int': tf.io.FixedLenFeature([], tf.int64, default_value=0),\n",
    "        }\n",
    "        rec = tf.io.parse_single_example(\n",
    "            proto, feature_description\n",
    "        )\n",
    "        shape = tf.sparse.to_dense(rec['shape'])\n",
    "        img = tf.reshape(tf.sparse.to_dense(rec['image']), shape)\n",
    "        label_int = rec['label_int']\n",
    "        return img, label_int\n",
    "    \n",
    "    def read_from_jpegfile(self, filename):\n",
    "        # same code as in 05_create_dataset/jpeg_to_tfrecord.py\n",
    "        img = tf.io.read_file(filename)\n",
    "        img = tf.image.decode_jpeg(img, channels=IMG_CHANNELS)\n",
    "        img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "        return img\n",
    "      \n",
    "    def preprocess(self, img):\n",
    "        return tf.image.resize_with_pad(img, IMG_HEIGHT, IMG_WIDTH)\n",
    "\n",
    "def create_preproc_dataset_plain(pattern):\n",
    "    preproc = _Preprocessor()\n",
    "    trainds = tf.data.TFRecordDataset(\n",
    "        [filename for filename in tf.io.gfile.glob(pattern)],\n",
    "        compression_type='GZIP'\n",
    "    ).map(preproc.read_from_tfr).map(\n",
    "        lambda img, label: (preproc.preprocess(img), label)\n",
    "    )                             \n",
    "    return trainds\n",
    "\n",
    "# note: addition of AUTOTUNE to the map() calls\n",
    "def create_preproc_dataset_parallelmap(pattern):\n",
    "    preproc = _Preprocessor()\n",
    "    def _preproc_img_label(img, label):\n",
    "        return (preproc.preprocess(img), label)\n",
    "    trainds = (\n",
    "        tf.data.TFRecordDataset(\n",
    "            [filename for filename in tf.io.gfile.glob(pattern)],\n",
    "            compression_type='GZIP'\n",
    "        )\n",
    "        .map(preproc.read_from_tfr, num_parallel_calls=AUTOTUNE)\n",
    "        .map(_preproc_img_label, num_parallel_calls=AUTOTUNE)\n",
    "    )\n",
    "    return trainds\n",
    "\n",
    "# note: splits the files into two halves and interleaves datasets\n",
    "def create_preproc_dataset_interleave(pattern, num_parallel=None):\n",
    "    preproc = _Preprocessor()\n",
    "    files = [filename for filename in tf.io.gfile.glob(pattern)]\n",
    "    if len(files) > 2:\n",
    "        print(\"Interleaving the reading of {} files.\".format(len(files)))\n",
    "        def _create_half_ds(x):\n",
    "            if x == 0:\n",
    "                half = files[:(len(files)//2)]\n",
    "            else:\n",
    "                half = files[(len(files)//2):]\n",
    "            return tf.data.TFRecordDataset(half,\n",
    "                                          compression_type='GZIP')\n",
    "        trainds = tf.data.Dataset.range(2).interleave(\n",
    "            _create_half_ds, num_parallel_calls=AUTOTUNE)\n",
    "    else:\n",
    "        trainds = tf.data.TFRecordDataset(files,\n",
    "                                         compression_type='GZIP')\n",
    "    def _preproc_img_label(img, label):\n",
    "        return (preproc.preprocess(img), label)\n",
    "    \n",
    "    trainds = (trainds\n",
    "               .map(preproc.read_from_tfr, num_parallel_calls=num_parallel)\n",
    "               .map(_preproc_img_label, num_parallel_calls=num_parallel)\n",
    "              )\n",
    "    return trainds\n",
    "\n",
    "def create_preproc_image(filename):\n",
    "    preproc = _Preprocessor()\n",
    "    img = preproc.read_from_jpegfile(filename)\n",
    "    return preproc.preprocess(img)\n",
    "\n",
    "class RandomColorDistortion(tf.keras.layers.Layer):\n",
    "    def __init__(self, contrast_range=[0.5, 1.5], \n",
    "                 brightness_delta=[-0.2, 0.2], **kwargs):\n",
    "        super(RandomColorDistortion, self).__init__(**kwargs)\n",
    "        self.contrast_range = contrast_range\n",
    "        self.brightness_delta = brightness_delta\n",
    "    \n",
    "    def call(self, images, training=None):\n",
    "        if not training:\n",
    "            return images\n",
    "        \n",
    "        contrast = np.random.uniform(\n",
    "            self.contrast_range[0], self.contrast_range[1])\n",
    "        brightness = np.random.uniform(\n",
    "            self.brightness_delta[0], self.brightness_delta[1])\n",
    "        \n",
    "        images = tf.image.adjust_contrast(images, contrast)\n",
    "        images = tf.image.adjust_brightness(images, brightness)\n",
    "        images = tf.clip_by_value(images, 0, 1)\n",
    "        return images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speeding up the reading of data\n",
    "\n",
    "To try it out, we'll simply read through the data several times and compute some quantity on the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop_through_dataset(ds, nepochs):\n",
    "    lowest_mean = tf.constant(1.)\n",
    "    for epoch in range(nepochs):\n",
    "        thresh = np.random.uniform(0.3, 0.7) # random threshold\n",
    "        count = 0\n",
    "        sumsofar = tf.constant(0.)\n",
    "        for (img, label) in ds:\n",
    "            # mean of channel values > thresh\n",
    "            mean = tf.reduce_mean(tf.where(img > thresh, img, 0))\n",
    "            sumsofar = sumsofar + mean\n",
    "            count = count + 1\n",
    "            if count%100 == 0:\n",
    "                print('.', end='')\n",
    "        mean = sumsofar/count\n",
    "        print(mean)\n",
    "        if mean < lowest_mean:\n",
    "            lowest_mean = mean\n",
    "    return lowest_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATTERN_SUFFIX, NUM_EPOCHS = '-0000[0123]-*', 5 # 4 files, 5 epochs\n",
    "#PATTERN_SUFFIX, NUM_EPOCHS = '-*', 20 # 16 files, 20 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".......tf.Tensor(0.23544756, shape=(), dtype=float32)\n",
      ".......tf.Tensor(0.22629641, shape=(), dtype=float32)\n",
      ".......tf.Tensor(0.21397452, shape=(), dtype=float32)\n",
      ".......tf.Tensor(0.11939337, shape=(), dtype=float32)\n",
      ".......tf.Tensor(0.22008048, shape=(), dtype=float32)\n",
      "CPU times: user 1min 11s, sys: 2.74 s, total: 1min 14s\n",
      "Wall time: 1min 3s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.11939337>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "ds = create_preproc_dataset_plain(\n",
    "    'gs://practical-ml-vision-book/flowers_tfr/train' + PATTERN_SUFFIX\n",
    ")\n",
    "loop_through_dataset(ds, NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.15482387, shape=(), dtype=float32)\n",
      "tf.Tensor(0.2181334, shape=(), dtype=float32)\n",
      "tf.Tensor(0.14096104, shape=(), dtype=float32)\n",
      "tf.Tensor(0.17421724, shape=(), dtype=float32)\n",
      "tf.Tensor(0.22375365, shape=(), dtype=float32)\n",
      "CPU times: user 3.26 s, sys: 1.26 s, total: 4.52 s\n",
      "Wall time: 7.44 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.14096104>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# parallel map\n",
    "ds = create_preproc_dataset_parallelmap(\n",
    "    'gs://practical-ml-vision-book/flowers_tfr/train' + PATTERN_SUFFIX\n",
    ")\n",
    "loop_through_dataset(ds, NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# with interleave\n",
    "ds = create_preproc_dataset_interleave(\n",
    "    'gs://practical-ml-vision-book/flowers_tfr/train' + PATTERN_SUFFIX,\n",
    "    num_parallel=None\n",
    ")\n",
    "loop_through_dataset(ds, NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# with interleave and parallel mpas\n",
    "ds = create_preproc_dataset_interleave(\n",
    "    'gs://practical-ml-vision-book/flowers_tfr/train' + PATTERN_SUFFIX,\n",
    "    num_parallel=AUTOTUNE\n",
    ")\n",
    "loop_through_dataset(ds, NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I did this, this is what I got:\n",
    "\n",
    "| Method             | CPU time    | Wall time    |\n",
    "| ------------------ | ----------- | ------------ |\n",
    "| Plain              | 4.52s       | 8.31s        |\n",
    "| Parallel Map       | 4.52s       | 7.44s        |\n",
    "| Interleave         | 4.98s       | 6.64s        |\n",
    "| Interleave+Parallel| 4.97s       | 6.25s        |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML model\n",
    "\n",
    "The computation above was pretty cheap -- just adding.\n",
    "What happens if we need a bit more complexity (gradient calc, etc.)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_simple_model(ds, nepochs):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Flatten(\n",
    "            input_shape=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)),\n",
    "        tf.keras.layers.Dense(256),\n",
    "        tf.keras.layers.Dense(len(CLASS_NAMES), activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "                loss=tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "                    from_logits=False),\n",
    "                metrics=['accuracy'])\n",
    "    model.fit(ds, epochs=nepochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ds = create_preproc_dataset_plain(\n",
    "    'gs://practical-ml-vision-book/flowers_tfr/train' + PATTERN_SUFFIX\n",
    ").batch(1)\n",
    "train_simple_model(ds, NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# parallel map\n",
    "ds = create_preproc_dataset_parallelmap(\n",
    "    'gs://practical-ml-vision-book/flowers_tfr/train' + PATTERN_SUFFIX\n",
    ").batch(1)\n",
    "train_simple_model(ds, NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# with interleave\n",
    "ds = create_preproc_dataset_interleave(\n",
    "    'gs://practical-ml-vision-book/flowers_tfr/train' + PATTERN_SUFFIX,\n",
    "    num_parallel=None\n",
    ").batch(1)\n",
    "train_simple_model(ds, NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# with interleave and parallel mpas\n",
    "ds = create_preproc_dataset_interleave(\n",
    "    'gs://practical-ml-vision-book/flowers_tfr/train' + PATTERN_SUFFIX,\n",
    "    num_parallel=AUTOTUNE\n",
    ").batch(1)\n",
    "train_simple_model(ds, NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that the improvement remains:\n",
    "\n",
    "| Method             | CPU time    | Wall time    |\n",
    "| ------------------ | ----------- | ------------ |\n",
    "| Plain              | 15.1s       | 17.8s        |\n",
    "| Parallel Map       | 14.6s       | 15.1s        |\n",
    "| Interleave         | 14.9s       | 13.6s        |\n",
    "| Interleave+Parallel| 15.1s       | 13.1s        |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speeding up the handling of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alias to the more efficient one\n",
    "def create_preproc_dataset(pattern):\n",
    "    return create_preproc_dataset_interleave(pattern, num_parallel=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# add prefetching\n",
    "ds = create_preproc_dataset(\n",
    "    'gs://practical-ml-vision-book/flowers_tfr/train' + PATTERN_SUFFIX\n",
    ").prefetch(AUTOTUNE).batch(1)\n",
    "train_simple_model(ds, NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Add batching of different sizes\n",
    "ds = create_preproc_dataset(\n",
    "    'gs://practical-ml-vision-book/flowers_tfr/train' + PATTERN_SUFFIX\n",
    ").prefetch(AUTOTUNE).batch(16)\n",
    "train_simple_model(ds, NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Add batching of different sizes\n",
    "ds = create_preproc_dataset(\n",
    "    'gs://practical-ml-vision-book/flowers_tfr/train' + PATTERN_SUFFIX\n",
    ").prefetch(AUTOTUNE).batch(32)\n",
    "train_simple_model(ds, NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# add caching: always do this optimization last.\n",
    "ds = create_preproc_dataset(\n",
    "    'gs://practical-ml-vision-book/flowers_tfr/train' + PATTERN_SUFFIX\n",
    ").cache().batch(32)\n",
    "train_simple_model(ds, NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# add caching: always do this optimization last.\n",
    "ds = create_preproc_dataset(\n",
    "    'gs://practical-ml-vision-book/flowers_tfr/train' + PATTERN_SUFFIX\n",
    ").prefetch(AUTOTUNE).cache().batch(32)\n",
    "train_simple_model(ds, NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# add caching: always do this optimization last.\n",
    "ds = create_preproc_dataset(\n",
    "    'gs://practical-ml-vision-book/flowers_tfr/train' + PATTERN_SUFFIX\n",
    ").cache().prefetch(AUTOTUNE).batch(32)\n",
    "train_simple_model(ds, NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding to the previous table:\n",
    "\n",
    "| Method             | CPU time    | Wall time    |\n",
    "| ------------------ | ----------- | ------------ |\n",
    "| Plain              | 15.1s       | 17.8s        |\n",
    "| Parallel Map       | 14.6s       | 15.1s        |\n",
    "| Interleave         | 14.9s       | 13.6s        |\n",
    "| Interleave+Parallel| 15.1s       | 13.1s        |\n",
    "| Interleave + Parallel, and then adding:         | - | - |\n",
    "| Prefetch           | 15.2s       | 13.3s        |\n",
    "| Batch size 16      | 6.43s       | 7.35s        |\n",
    "| Batch size 32      | 6.03s       | 6.98s        |\n",
    "| Interleave + Parallel + batchsize 32, and then adding: | - | - |\n",
    "| Cache              | 2.49s       | 2.93s        |\n",
    "| Prefetch + Cache   | 3.26s       | 3.78s        |\n",
    "| Cache + Prefetch   | 2.48s       | 3.09s        |\n",
    "\n",
    "So, the best option is:\n",
    "<pre>\n",
    "ds = create_preproc_dataset_interleave(pattern, num_parallel=AUTOTUNE).cache().batch(32)\n",
    "</pre>\n",
    "closely followed by:\n",
    "<pre>\n",
    "ds = create_preproc_dataset_interleave(pattern, num_parallel=AUTOTUNE).cache().prefetch(AUTOTUNE).batch(32)\n",
    "</pre>\n",
    "It's likely that prefetching will help with more complex models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply efficiency improvements\n",
    "\n",
    "Interleaving, parallel calls, prefetch, cache, batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATTERN_SUFFIX, NUM_EPOCHS = '-0000[0123]-*', 3 # small\n",
    "PATTERN_SUFFIX, NUM_EPOCHS = '-*', 20 # full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(batch_size = 32,\n",
    "                       lrate = 0.001,\n",
    "                       l1 = 0.,\n",
    "                       l2 = 0.,\n",
    "                       num_hidden = 16):\n",
    "    regularizer = tf.keras.regularizers.l1_l2(l1, l2)\n",
    "    \n",
    "    train_dataset = create_preproc_dataset_interleave(\n",
    "        'gs://practical-ml-vision-book/flowers_tfr/train' + PATTERN_SUFFIX,\n",
    "        num_parallel=AUTOTUNE\n",
    "    ).prefetch(AUTOTUNE).batch(batch_size)\n",
    "    eval_dataset = create_preproc_dataset_interleave(\n",
    "        'gs://practical-ml-vision-book/flowers_tfr/valid' + PATTERN_SUFFIX,\n",
    "        num_parallel=AUTOTUNE\n",
    "    ).prefetch(AUTOTUNE).batch(batch_size)\n",
    "\n",
    "    layers = [\n",
    "      tf.keras.layers.experimental.preprocessing.RandomCrop(\n",
    "          height=IMG_HEIGHT//2, width=IMG_WIDTH//2,\n",
    "          input_shape=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS),\n",
    "          name='random/center_crop'\n",
    "      ),\n",
    "      tf.keras.layers.experimental.preprocessing.RandomFlip(\n",
    "          mode='horizontal',\n",
    "          name='random_lr_flip/none'\n",
    "      ),\n",
    "      RandomColorDistortion(name='random_contrast_brightness/none'),\n",
    "      hub.KerasLayer(\n",
    "          \"https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4\", \n",
    "          trainable=False,\n",
    "          name='mobilenet_embedding'),\n",
    "      tf.keras.layers.Dense(num_hidden,\n",
    "                            kernel_regularizer=regularizer, \n",
    "                            activation=tf.keras.activations.relu,\n",
    "                            name='dense_hidden'),\n",
    "      tf.keras.layers.Dense(len(CLASS_NAMES), \n",
    "                            kernel_regularizer=regularizer,\n",
    "                            activation='softmax',\n",
    "                            name='flower_prob')\n",
    "    ]\n",
    "\n",
    "    model = tf.keras.Sequential(layers, name='flower_classification')\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lrate),\n",
    "                loss=tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "                    from_logits=False),\n",
    "                metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    history = model.fit(train_dataset, validation_data=eval_dataset, epochs=NUM_EPOCHS)\n",
    "    training_plot(['loss', 'accuracy'], history)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 929
    },
    "id": "jlxxxpeaT6ea",
    "outputId": "ad4f09e8-bc33-4c92-dc47-5fc73ec12f9c"
   },
   "outputs": [],
   "source": [
    "model = train_and_evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these efficiencies, an epoch takes around 60s (as compared to 120s). So 2x as fast."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model, then load it to make predictions\n",
    "\n",
    "This way, we don't have to have the model in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "shutil.rmtree('export', ignore_errors=True)\n",
    "os.mkdir('export')\n",
    "model.save('export/flowers_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls export/flowers_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!saved_model_cli show --tag_set serve --signature_def serving_default --dir export/flowers_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 682
    },
    "id": "HEh_vh0vvnNd",
    "outputId": "e43ebba2-fe2b-4a53-8b94-32598c021498"
   },
   "outputs": [],
   "source": [
    "# Call model.predict() on a few images in evaluation dataset\n",
    "def plot_predictions(model, pattern):\n",
    "    dataset = create_preproc_dataset_plain(pattern)\n",
    "    f, ax = plt.subplots(3, 5, figsize=(25,15))\n",
    "    for idx, (img, label) in enumerate(dataset.take(15)):\n",
    "        ax[idx//5, idx%5].imshow((img.numpy()));\n",
    "        batch_image = tf.reshape(img, [1, IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS])\n",
    "        batch_pred = model.predict(batch_image)\n",
    "        pred = batch_pred[0]\n",
    "        label = CLASS_NAMES[label.numpy()]\n",
    "        pred_label_index = tf.math.argmax(pred).numpy()\n",
    "        pred_label = CLASS_NAMES[pred_label_index]\n",
    "        prob = pred[pred_label_index]\n",
    "        ax[idx//5, idx%5].set_title('{}: {} ({:.4f})'.format(label, pred_label, prob))\n",
    "\n",
    "serving_model = tf.keras.models.load_model('export/flowers_model')\n",
    "plot_predictions(serving_model, 'gs://practical-ml-vision-book/flowers_tfr/valid-*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## but actually, for prediction, we won't have TensorFlow Records.\n",
    "## this is how we'd predict for individual images\n",
    "## (actually, we'll create a serving function: we'll look at that in a later chapter)\n",
    "filenames = [\n",
    "    'gs://cloud-ml-data/img/flower_photos/dandelion/9818247_e2eac18894.jpg',\n",
    "    'gs://cloud-ml-data/img/flower_photos/dandelion/9853885425_4a82356f1d_m.jpg',\n",
    "    'gs://cloud-ml-data/img/flower_photos/dandelion/98992760_53ed1d26a9.jpg',\n",
    "    'gs://cloud-ml-data/img/flower_photos/dandelion/9939430464_5f5861ebab.jpg',\n",
    "    'gs://cloud-ml-data/img/flower_photos/dandelion/9965757055_ff01b5ee6f_n.jpg'\n",
    "]\n",
    "label = 'dandelion'\n",
    "input_images = [create_preproc_image(f) for f in filenames]\n",
    "f, ax = plt.subplots(1, 5, figsize=(15,15))\n",
    "for idx, img in enumerate(input_images):\n",
    "    ax[idx].imshow((img.numpy()));\n",
    "    batch_image = tf.reshape(img, [1, IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS])\n",
    "    batch_pred = serving_model.predict(batch_image)\n",
    "    pred = batch_pred[0]\n",
    "    pred_label_index = tf.math.argmax(pred).numpy()\n",
    "    pred_label = CLASS_NAMES[pred_label_index]\n",
    "    prob = pred[pred_label_index]\n",
    "    ax[idx].set_title('{}: {} ({:.4f})'.format(label, pred_label, prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Duu8mX3iXANE"
   },
   "source": [
    "## License\n",
    "Copyright 2020 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "5UOm2etrwYCs"
   ],
   "name": "03a_transfer_learning.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "name": "tf2-2-3-gpu.2-3.m59",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-3-gpu.2-3:m59"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
