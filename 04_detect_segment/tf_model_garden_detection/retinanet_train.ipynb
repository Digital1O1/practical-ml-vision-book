{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet gin-config\n",
    "!pip install --quiet tensorflow-hub\n",
    "!pip install --quiet tensorflow-addons\n",
    "!pip install --quiet sentencepiece\n",
    "!pip install --quiet pycocotools\n",
    "!pip install --quiet opencv-python-headless\n",
    "!pip install --quiet seqeval\n",
    "!pip install --quiet tensorflow_text\n",
    "!pip install --quiet sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version 2.5.0\n"
     ]
    }
   ],
   "source": [
    "import sys, re, math, time, os\n",
    "sys.path.append('../../../models') # Path to Tensorflow model garden installation directory\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import pprint as pp\n",
    "AUTO = tf.data.AUTOTUNE\n",
    "print(\"Tensorflow version\", tf.__version__)\n",
    "\n",
    "# Tensorflow Model Garden imports\n",
    "from official.core import task_factory\n",
    "from official.core import exp_factory\n",
    "from official.common import registry_imports\n",
    "from official.core import config_definitions as cfg\n",
    "from official.modeling import optimization\n",
    "from official.vision.beta.tasks import retinanet as retinanet_task\n",
    "from official.vision.beta.configs import retinanet as retinanet_cfg\n",
    "from official.vision.beta.configs import backbones as backbones_cfg\n",
    "from official.modeling.hyperparams import params_dict as params_dict\n",
    "from official.core import train_lib\n",
    "from official.vision.beta.serving import export_saved_model_lib\n",
    "\n",
    "# for debugging\n",
    "from official.vision.beta.ops import preprocess_ops\n",
    "from official.vision.beta.ops import box_ops\n",
    "\n",
    "import official as modelgarden\n",
    "\n",
    "#from official.vision.detection.configs import factory as tf_garden_config_factory\n",
    "#from official.modeling.hyperparams import params_dict as tf_garden_params_dict\n",
    "#from official.vision.detection.modeling import factory as tf_garden_model_factory\n",
    "#from official.vision.detection.dataloader import input_reader as tf_garden_input_reader\n",
    "#from official.vision.detection.dataloader import mode_keys as tf_garden_mode_keys\n",
    "#from official.vision.detection.executor.detection_executor import DetectionDistributedExecutor as tf_garden_training_loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TPU / GPU detection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initializing the TPU system: martin-tpuv3-8-tf25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initializing the TPU system: martin-tpuv3-8-tf25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Clearing out eager caches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Clearing out eager caches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished initializing TPU system.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished initializing TPU system.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Found TPU system:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Found TPU system:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Workers: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Workers: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPLICAS:  8\n"
     ]
    }
   ],
   "source": [
    "strategy = tf.distribute.get_strategy()\n",
    "\n",
    "try: # detect TPUs\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n",
    "    strategy = tf.distribute.TPUStrategy(tpu)\n",
    "    #policy = tf.keras.mixed_precision.experimental.Policy('mixed_bfloat16')\n",
    "    #tf.keras.mixed_precision.experimental.set_policy(policy)\n",
    "except ValueError: # detect GPUs or multi-GPU machines\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset:\n",
      "Found 24 TFRecord files.\n",
      "11544 images\n",
      "Steps per epoch: 45\n",
      "\n",
      "Validation dataset:\n",
      "Found 8 TFRecord files.\n",
      "3832 images\n",
      "Validation steps: 14\n",
      "\n",
      "Global batch size: 256\n",
      "Model dir: gs://ml1-demo-martin/arthropod_jobs/job1625758868\n"
     ]
    }
   ],
   "source": [
    "TRAIN_DATA_PATH_PATTERN = 'gs://practical-ml-vision-book/arthropod_detection_tfr/size_w1024px/*.train.tfrec'\n",
    "VALID_DATA_PATH_PATTERN = 'gs://practical-ml-vision-book/arthropod_detection_tfr/size_w1024px/*.test.tfrec'\n",
    "#SPINET_MOBILE_CHECKPOINT = '../../../ckpt/'\n",
    "SPINET_MOBILE_CHECKPOINT = 'gs://practical-ml-vision-book/arthropod_detection_tfr/spinenet_mobile_checkpoint/'\n",
    "\n",
    "BATCH_SIZE = 32 * strategy.num_replicas_in_sync\n",
    "\n",
    "EPOCHS = 80\n",
    "\n",
    "MODEL_DIR = 'gs://ml1-demo-martin/arthropod_jobs/job' + str(int(time.time()))\n",
    "#MODEL_DIR = \"gs://ml1-demo-martin/arthropod_jobs/job1624685364\"\n",
    "\n",
    "RAW_CLASSES = ['Lepidoptera', 'Hymenoptera', 'Hemiptera', 'Odonata', 'Diptera', 'Araneae', 'Coleoptera',\n",
    "               '_truncated', '_blurred', '_occluded', ]\n",
    "CLASSES = [klass for klass in RAW_CLASSES if klass not in ['_truncated', '_blurred', '_occluded']]\n",
    "\n",
    "#PAD_MAX_BOXES = 50\n",
    "\n",
    "# Lepidoptera = butterfies and moths\n",
    "# Hymenoptera = wasps, bees and ants\n",
    "# Hemiptera = true bugs (cicadas, aphids, shield bugs, ...)\n",
    "# Odonata = dragonflies\n",
    "# Diptera = fies\n",
    "# Araneae = spiders\n",
    "# Coleoptera = beetles\n",
    "\n",
    "# NOT IN DATASET\n",
    "# Orthoptera = grasshoppers\n",
    "\n",
    "def count_data_items(filenames):\n",
    "    # the number of data items is written in the name of the .tfrec files, i.e. flowers00-230.tfrec = 230 data items\n",
    "    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n",
    "    return int(np.sum(n))\n",
    "\n",
    "TRAIN_FILENAMES = tf.io.gfile.glob(TRAIN_DATA_PATH_PATTERN)\n",
    "VALID_FILENAMES = tf.io.gfile.glob(VALID_DATA_PATH_PATTERN)\n",
    "\n",
    "print(\"Training dataset:\")\n",
    "print(f\"Found {len(TRAIN_FILENAMES)} TFRecord files.\")\n",
    "NB_TRAIN_IMAGES = count_data_items(TRAIN_FILENAMES)\n",
    "STEPS_PER_EPOCH = NB_TRAIN_IMAGES // BATCH_SIZE\n",
    "print(f\"{NB_TRAIN_IMAGES} images\")\n",
    "print(\"Steps per epoch:\", STEPS_PER_EPOCH)\n",
    "print()\n",
    "print(\"Validation dataset:\")\n",
    "print(f\"Found {len(VALID_FILENAMES)} TFRecord files.\")\n",
    "NB_VALID_IMAGES = count_data_items(VALID_FILENAMES)\n",
    "VALID_STEPS = NB_VALID_IMAGES // BATCH_SIZE\n",
    "print(f\"{NB_VALID_IMAGES} images\")\n",
    "print(\"Validation steps:\", VALID_STEPS)\n",
    "print()\n",
    "print(\"Global batch size:\", BATCH_SIZE)\n",
    "print(\"Model dir:\", MODEL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_decorations(ax):\n",
    "    ax.axes.get_xaxis().set_visible(False)\n",
    "    ax.axes.get_yaxis().set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    \n",
    "\n",
    "def display_detections(images, offsets, resizes, detections, classnames, ground_truth_boxes=[]):\n",
    "    # scale and offset the detected boxes back to original image coordinates\n",
    "    boxes   = [[ (x,y,w,h)  for _, x, y, w, h, score, klass in detection_list] for detection_list in detections]\n",
    "    boxes   = [[ (x-ofs[1], y-ofs[0], w, h) for x,y,w,h in boxlist ] for boxlist, ofs in zip(boxes, offsets)]\n",
    "    boxes   = [[ (x*rsz, y*rsz, w*rsz, h*rsz) for x,y,w,h in boxlist ] for boxlist, rsz in zip(boxes, resizes)]\n",
    "    classes = [[ int(klass) for _, x, y, w, h, score, klass in detection_list] for detection_list in detections]\n",
    "    scores  = [[ score      for _, x, y, w, h, score, klass in detection_list] for detection_list in detections]\n",
    "    display_with_boxes(images, boxes, classes, scores, classnames, ground_truth_boxes)\n",
    "    \n",
    "    \n",
    "# images, boxes and classes must have the same number of elements\n",
    "# scores can be en empty list []. If it is not empty, it must also\n",
    "# have the same number of elements.\n",
    "# classnames is the list of possible classes (strings)\n",
    "def display_with_boxes(images, boxes, classes, scores, classnames, ground_truth_boxes=[]):\n",
    "    N = len(images)\n",
    "    sqrtN = int(math.ceil(math.sqrt(N)))\n",
    "    aspect = sum([im.shape[1]/im.shape[0] for im in images])/len(images) # mean aspect ratio of images\n",
    "    fig = plt.figure(figsize=(15,15/aspect), frameon=False)\n",
    "    \n",
    "    for k in range(N):\n",
    "        ax = plt.subplot(sqrtN, sqrtN, k+1)\n",
    "        no_decorations(ax)\n",
    "        plt.imshow(images[k])\n",
    "        \n",
    "        if ground_truth_boxes:\n",
    "            for box in ground_truth_boxes[k]:\n",
    "                x, y, w, h = (box[0], box[1], box[2]-box[0], box[3]-box[1]) # convert x1 y1 x2 y2 into xywh\n",
    "                #x, y, w, h = (box[0], box[1], box[2], box[3])\n",
    "                rect = mpl.patches.Rectangle((x, y),w,h,linewidth=4,edgecolor='#FFFFFFA0',facecolor='none')\n",
    "                ax.add_patch(rect)\n",
    "\n",
    "        for i, (box, klass) in enumerate(zip(boxes[k], classes[k])):\n",
    "            x, y, w, h = (box[0], box[1], box[2]-box[0], box[3]-box[1]) # convert x1 y1 x2 y2 into xywh\n",
    "            #x, y, w, h = (box[0], box[1], box[2], box[3])\n",
    "            #label = classnames[klass-1] # predicted classes are 1-based\n",
    "            label = classnames[klass]\n",
    "            if scores:\n",
    "                label += ' ' + str(int(scores[k][i]*100)) + '%' \n",
    "            rect = mpl.patches.Rectangle((x, y),w,h,linewidth=4,edgecolor='#00000080',facecolor='none')\n",
    "            ax.add_patch(rect)\n",
    "            rect = mpl.patches.Rectangle((x, y),w,h,linewidth=2,edgecolor='#FFFF00FF',facecolor='none')\n",
    "            ax.add_patch(rect)\n",
    "            plt.text(x, y, label, size=16, ha=\"left\", va=\"top\", color='#FFFF00FF',\n",
    "                     bbox=dict(boxstyle=\"round\", ec='#00000080', fc='#0000004E', linewidth=3) )\n",
    "            plt.text(x, y, label, size=16, ha=\"left\", va=\"top\", color='#FFFF00FF',\n",
    "                     bbox=dict(boxstyle=\"round\", ec='#FFFF00FF', fc='#0000004E', linewidth=1.5) )\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(wspace=0.02, hspace=0.02)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_SIZE = [384, 384]\n",
    "#params = exp_factory.get_exp_config('retinanet_resnetfpn_coco')\n",
    "params = exp_factory.get_exp_config('retinanet')\n",
    "params.task.model.num_classes = len(CLASSES)+1\n",
    "params.task.model.input_size = [*OUTPUT_SIZE, 3] # this automatically configures the input reader to random crop training images ?\n",
    "#pp.pprint(params.as_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "{'runtime': {'all_reduce_alg': None,\n",
      "             'batchnorm_spatial_persistent': False,\n",
      "             'dataset_num_private_threads': None,\n",
      "             'default_shard_dim': -1,\n",
      "             'distribution_strategy': 'mirrored',\n",
      "             'enable_xla': False,\n",
      "             'gpu_thread_mode': None,\n",
      "             'loss_scale': None,\n",
      "             'mixed_precision_dtype': None,\n",
      "             'num_cores_per_replica': 1,\n",
      "             'num_gpus': 0,\n",
      "             'num_packs': 1,\n",
      "             'per_gpu_thread_count': 0,\n",
      "             'run_eagerly': False,\n",
      "             'task_index': -1,\n",
      "             'tpu': None,\n",
      "             'tpu_enable_xla_dynamic_padder': None,\n",
      "             'worker_hosts': None},\n",
      " 'task': {'annotation_file': None,\n",
      "          'init_checkpoint': 'gs://practical-ml-vision-book/arthropod_detection_tfr/spinenet_mobile_checkpoint/',\n",
      "          'init_checkpoint_modules': 'backbone',\n",
      "          'losses': {'box_loss_weight': 50,\n",
      "                     'focal_loss_alpha': 0.25,\n",
      "                     'focal_loss_gamma': 1.5,\n",
      "                     'huber_loss_delta': 0.1,\n",
      "                     'l2_weight_decay': 0.0},\n",
      "          'model': {'anchor': {'anchor_size': 4.0,\n",
      "                               'aspect_ratios': [0.5, 1.0, 2.0],\n",
      "                               'num_scales': 3},\n",
      "                    'backbone': {'spinenet_mobile': {'expand_ratio': 6,\n",
      "                                                     'model_id': '49',\n",
      "                                                     'se_ratio': 0.2,\n",
      "                                                     'stochastic_depth_drop_rate': 0.0},\n",
      "                                 'type': 'spinenet_mobile'},\n",
      "                    'decoder': {'fpn': {'num_filters': 256,\n",
      "                                        'use_separable_conv': False},\n",
      "                                'type': 'fpn'},\n",
      "                    'detection_generator': {'max_num_detections': 100,\n",
      "                                            'nms_iou_threshold': 0.5,\n",
      "                                            'pre_nms_score_threshold': 0.05,\n",
      "                                            'pre_nms_top_k': 5000,\n",
      "                                            'use_batched_nms': False},\n",
      "                    'head': {'num_convs': 4,\n",
      "                             'num_filters': 256,\n",
      "                             'use_separable_conv': False},\n",
      "                    'input_size': [384, 384, 3],\n",
      "                    'max_level': 7,\n",
      "                    'min_level': 3,\n",
      "                    'norm_activation': {'activation': 'relu',\n",
      "                                        'norm_epsilon': 0.001,\n",
      "                                        'norm_momentum': 0.99,\n",
      "                                        'use_sync_bn': True},\n",
      "                    'num_classes': 8},\n",
      "          'per_category_metrics': False,\n",
      "          'train_data': {'block_length': 1,\n",
      "                         'cache': False,\n",
      "                         'cycle_length': None,\n",
      "                         'decoder': {'simple_decoder': {'regenerate_source_id': False},\n",
      "                                     'type': 'simple_decoder'},\n",
      "                         'deterministic': None,\n",
      "                         'drop_remainder': True,\n",
      "                         'dtype': 'bfloat16',\n",
      "                         'enable_tf_data_service': False,\n",
      "                         'file_type': 'tfrecord',\n",
      "                         'global_batch_size': 256,\n",
      "                         'input_path': 'gs://practical-ml-vision-book/arthropod_detection_tfr/size_w1024px/*.train.tfrec',\n",
      "                         'is_training': True,\n",
      "                         'parser': {'aug_rand_hflip': True,\n",
      "                                    'aug_scale_max': 2.0,\n",
      "                                    'aug_scale_min': 0.7,\n",
      "                                    'match_threshold': 0.5,\n",
      "                                    'max_num_instances': 100,\n",
      "                                    'num_channels': 3,\n",
      "                                    'skip_crowd_during_training': True,\n",
      "                                    'unmatched_threshold': 0.5},\n",
      "                         'seed': None,\n",
      "                         'sharding': True,\n",
      "                         'shuffle_buffer_size': 10000,\n",
      "                         'tf_data_service_address': None,\n",
      "                         'tf_data_service_job_name': None,\n",
      "                         'tfds_as_supervised': False,\n",
      "                         'tfds_data_dir': '',\n",
      "                         'tfds_name': '',\n",
      "                         'tfds_skip_decoding_feature': '',\n",
      "                         'tfds_split': ''},\n",
      "          'validation_data': {'block_length': 1,\n",
      "                              'cache': False,\n",
      "                              'cycle_length': None,\n",
      "                              'decoder': {'simple_decoder': {'regenerate_source_id': False},\n",
      "                                          'type': 'simple_decoder'},\n",
      "                              'deterministic': None,\n",
      "                              'drop_remainder': True,\n",
      "                              'dtype': 'bfloat16',\n",
      "                              'enable_tf_data_service': False,\n",
      "                              'file_type': 'tfrecord',\n",
      "                              'global_batch_size': 256,\n",
      "                              'input_path': 'gs://practical-ml-vision-book/arthropod_detection_tfr/size_w1024px/*.test.tfrec',\n",
      "                              'is_training': False,\n",
      "                              'parser': {'aug_rand_hflip': False,\n",
      "                                         'aug_scale_max': 1.0,\n",
      "                                         'aug_scale_min': 1.0,\n",
      "                                         'match_threshold': 0.5,\n",
      "                                         'max_num_instances': 100,\n",
      "                                         'num_channels': 3,\n",
      "                                         'skip_crowd_during_training': True,\n",
      "                                         'unmatched_threshold': 0.5},\n",
      "                              'seed': None,\n",
      "                              'sharding': True,\n",
      "                              'shuffle_buffer_size': 10000,\n",
      "                              'tf_data_service_address': None,\n",
      "                              'tf_data_service_job_name': None,\n",
      "                              'tfds_as_supervised': False,\n",
      "                              'tfds_data_dir': '',\n",
      "                              'tfds_name': '',\n",
      "                              'tfds_skip_decoding_feature': '',\n",
      "                              'tfds_split': ''}},\n",
      " 'trainer': {'allow_tpu_summary': False,\n",
      "             'best_checkpoint_eval_metric': '',\n",
      "             'best_checkpoint_export_subdir': '',\n",
      "             'best_checkpoint_metric_comp': 'higher',\n",
      "             'checkpoint_interval': 360,\n",
      "             'continuous_eval_timeout': 3600,\n",
      "             'eval_tf_function': True,\n",
      "             'eval_tf_while_loop': False,\n",
      "             'loss_upper_bound': 1000000.0,\n",
      "             'max_to_keep': 5,\n",
      "             'optimizer_config': {'ema': None,\n",
      "                                  'learning_rate': {'stepwise': {'boundaries': [675,\n",
      "                                                                                1350,\n",
      "                                                                                2025,\n",
      "                                                                                2700,\n",
      "                                                                                3375],\n",
      "                                                                 'name': 'PiecewiseConstantDecay',\n",
      "                                                                 'values': [0.016,\n",
      "                                                                            0.008,\n",
      "                                                                            0.004,\n",
      "                                                                            0.002,\n",
      "                                                                            0.001,\n",
      "                                                                            0.0005]},\n",
      "                                                    'type': 'stepwise'},\n",
      "                                  'optimizer': {'sgd': {'clipnorm': None,\n",
      "                                                        'clipvalue': None,\n",
      "                                                        'decay': 0.0,\n",
      "                                                        'global_clipnorm': None,\n",
      "                                                        'momentum': 0.9,\n",
      "                                                        'name': 'SGD',\n",
      "                                                        'nesterov': False},\n",
      "                                                'type': 'sgd'},\n",
      "                                  'warmup': {'type': None}},\n",
      "             'recovery_begin_steps': 0,\n",
      "             'recovery_max_trials': 0,\n",
      "             'steps_per_loop': 45,\n",
      "             'summary_interval': 45,\n",
      "             'train_steps': 3600,\n",
      "             'train_tf_function': True,\n",
      "             'train_tf_while_loop': True,\n",
      "             'validation_interval': 360,\n",
      "             'validation_steps': 14}}\n"
     ]
    }
   ],
   "source": [
    "train_data_cfg=retinanet_cfg.DataConfig(\n",
    "    input_path=TRAIN_DATA_PATH_PATTERN,\n",
    "    is_training=True,\n",
    "    global_batch_size=BATCH_SIZE,\n",
    "    parser=retinanet_cfg.Parser(aug_rand_hflip=True, aug_scale_min=0.7, aug_scale_max=2.0))\n",
    "\n",
    "valid_data_cfg=retinanet_cfg.DataConfig(\n",
    "    input_path=VALID_DATA_PATH_PATTERN,\n",
    "    is_training=False,\n",
    "    global_batch_size=BATCH_SIZE)\n",
    "\n",
    "trainer_cfg=cfg.TrainerConfig(\n",
    "    train_steps=EPOCHS * STEPS_PER_EPOCH,\n",
    "    validation_steps=VALID_STEPS,\n",
    "    validation_interval=8*STEPS_PER_EPOCH,\n",
    "    steps_per_loop=STEPS_PER_EPOCH,\n",
    "    summary_interval=STEPS_PER_EPOCH,\n",
    "    checkpoint_interval=8*STEPS_PER_EPOCH)\n",
    "\n",
    "MUL=3\n",
    "optim_cfg = optimization.OptimizationConfig({\n",
    "    'optimizer': {\n",
    "                  'type': 'sgd',\n",
    "                  'sgd': {'momentum': 0.9}},\n",
    "                  #'type': 'adam'},\n",
    "    #'learning_rate': {'type': 'exponential',\n",
    "    #                  'exponential': {'initial_learning_rate': 0.001 * strategy.num_replicas_in_sync,\n",
    "    #                                  'decay_steps':15,\n",
    "    #                                  'decay_rate':0.96,\n",
    "    #                                  'offset': 0}\n",
    "    #                 },\n",
    "    'learning_rate': {'type': 'stepwise',\n",
    "                      'stepwise': {'boundaries': [MUL * 5 * STEPS_PER_EPOCH,\n",
    "                                                  MUL * 10 * STEPS_PER_EPOCH,\n",
    "                                                  MUL * 15 * STEPS_PER_EPOCH,\n",
    "                                                  MUL * 20 * STEPS_PER_EPOCH,\n",
    "                                                  MUL * 25 * STEPS_PER_EPOCH],\n",
    "                                   'values': [0.016, #0.01,\n",
    "                                              0.008, #0.005,\n",
    "                                              0.004, #0.0025,\n",
    "                                              0.002, #0.001,\n",
    "                                              0.001, #0.0005,\n",
    "                                              0.0005]} #0.00025]}\n",
    "                     },\n",
    "    #'warmup': {'type': 'linear','linear': {'warmup_steps': 5*STEPS_PER_EPOCH, 'warmup_learning_rate': 0.00001}}\n",
    "})\n",
    "\n",
    "params = params_dict.override_params_dict(params, {'task': {'train_data': train_data_cfg.as_dict(),\n",
    "                                                           'validation_data': valid_data_cfg.as_dict()}}, is_strict=True)\n",
    "trainer_cfg = params_dict.override_params_dict(trainer_cfg, {'optimizer_config': optim_cfg.as_dict()}, is_strict=True)\n",
    "params = params_dict.override_params_dict(params, {'trainer': trainer_cfg.as_dict()}, is_strict=True)\n",
    "#RESNET50_CHECKPOINT = 'https://storage.cloud.google.com/cloud-tpu-checkpoints/model-garden-vision/detection/resnet50-2018-02-07.tar.gz'\n",
    "#RESNET50_CHECKPOINT = 'gs://cloud-tpu-checkpoints/retinanet/resnet50-checkpoint-2018-02-07'\n",
    "#RESNET50_CHECKPOINT = 'gs://cloud-tpu-checkpoints/retinanet/resnet50-checkpoint-2018-02-07'\n",
    "#RESNET50_CHECKPOINT = 'gs://cloud-tpu-checkpoints/model-garden-vision/detection/resnet50-2018-02-07.tar.gz'\n",
    "#RESNET50_CHECKPOINT = 'gs://cloud-tpu-checkpoints/resnet/resnet50/'\n",
    "#RESNET50_CHECKPOINT = 'gs://cloud-tpu-checkpoints/detection/retinanet/r50-fpn.tar.gz'\n",
    "backbone_config = backbones_cfg.Backbone(type='spinenet_mobile', spinenet_mobile=backbones_cfg.SpineNetMobile())\n",
    "params = params_dict.override_params_dict(params, {'task': {'model': {'backbone': backbone_config.as_dict()}}}, is_strict=True)\n",
    "params = params_dict.override_params_dict(params, {'task': {'init_checkpoint': SPINET_MOBILE_CHECKPOINT}}, is_strict=True)\n",
    "params = params_dict.override_params_dict(params, {'task': {'init_checkpoint_modules': 'backbone'}}, is_strict=True)\n",
    "\n",
    "# this works as well:\n",
    "# params.task.train_data = train_data\n",
    "#params.task.validation_data = None\n",
    "print(\"-------------------------------------\")\n",
    "pp.pprint(params.as_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with strategy.scope():\n",
    "task = task_factory.get_task(params.task, logging_dir=MODEL_DIR)\n",
    "    # this works too:\n",
    "    # task = retinanet_task.RetinaNetTask(params.task)\n",
    "#task.build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'block_length': 1,\n",
      " 'cache': False,\n",
      " 'cycle_length': None,\n",
      " 'decoder': {'simple_decoder': {'regenerate_source_id': False},\n",
      "             'type': 'simple_decoder'},\n",
      " 'deterministic': None,\n",
      " 'drop_remainder': True,\n",
      " 'dtype': 'bfloat16',\n",
      " 'enable_tf_data_service': False,\n",
      " 'file_type': 'tfrecord',\n",
      " 'global_batch_size': 256,\n",
      " 'input_path': 'gs://practical-ml-vision-book/arthropod_detection_tfr/size_w1024px/*.train.tfrec',\n",
      " 'is_training': True,\n",
      " 'parser': {'aug_rand_hflip': True,\n",
      "            'aug_scale_max': 2.0,\n",
      "            'aug_scale_min': 0.7,\n",
      "            'match_threshold': 0.5,\n",
      "            'max_num_instances': 100,\n",
      "            'num_channels': 3,\n",
      "            'skip_crowd_during_training': True,\n",
      "            'unmatched_threshold': 0.5},\n",
      " 'seed': None,\n",
      " 'sharding': True,\n",
      " 'shuffle_buffer_size': 10000,\n",
      " 'tf_data_service_address': None,\n",
      " 'tf_data_service_job_name': None,\n",
      " 'tfds_as_supervised': False,\n",
      " 'tfds_data_dir': '',\n",
      " 'tfds_name': '',\n",
      " 'tfds_skip_decoding_feature': '',\n",
      " 'tfds_split': ''}\n",
      "{'block_length': 1,\n",
      " 'cache': False,\n",
      " 'cycle_length': None,\n",
      " 'decoder': {'simple_decoder': {'regenerate_source_id': False},\n",
      "             'type': 'simple_decoder'},\n",
      " 'deterministic': None,\n",
      " 'drop_remainder': True,\n",
      " 'dtype': 'bfloat16',\n",
      " 'enable_tf_data_service': False,\n",
      " 'file_type': 'tfrecord',\n",
      " 'global_batch_size': 256,\n",
      " 'input_path': 'gs://practical-ml-vision-book/arthropod_detection_tfr/size_w1024px/*.test.tfrec',\n",
      " 'is_training': False,\n",
      " 'parser': {'aug_rand_hflip': False,\n",
      "            'aug_scale_max': 1.0,\n",
      "            'aug_scale_min': 1.0,\n",
      "            'match_threshold': 0.5,\n",
      "            'max_num_instances': 100,\n",
      "            'num_channels': 3,\n",
      "            'skip_crowd_during_training': True,\n",
      "            'unmatched_threshold': 0.5},\n",
      " 'seed': None,\n",
      " 'sharding': True,\n",
      " 'shuffle_buffer_size': 10000,\n",
      " 'tf_data_service_address': None,\n",
      " 'tf_data_service_job_name': None,\n",
      " 'tfds_as_supervised': False,\n",
      " 'tfds_data_dir': '',\n",
      " 'tfds_name': '',\n",
      " 'tfds_skip_decoding_feature': '',\n",
      " 'tfds_split': ''}\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(train_data_cfg.as_dict())\n",
    "pp.pprint(valid_data_cfg.as_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MapDataset shapes: {source_id: (), image: (None, None, 3), height: (), width: (), groundtruth_classes: (None,), groundtruth_is_crowd: (None,), groundtruth_area: (None,), groundtruth_boxes: (None, 4)}, types: {source_id: tf.string, image: tf.uint8, height: tf.int64, width: tf.int64, groundtruth_classes: tf.int64, groundtruth_is_crowd: tf.bool, groundtruth_area: tf.float32, groundtruth_boxes: tf.float32}>\n"
     ]
    }
   ],
   "source": [
    "#DEBUG\n",
    "filenames = tf.io.gfile.glob(TRAIN_DATA_PATH_PATTERN)\n",
    "ds = tf.data.TFRecordDataset(filenames)\n",
    "dec = modelgarden.vision.beta.dataloaders.tf_example_decoder.TfExampleDecoder()\n",
    "ds = ds.map(dec.decode)\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEBUG\n",
    "def _parse_train_data(data):\n",
    "    \"\"\"Parses data for training and evaluation.\"\"\"\n",
    "    classes = data['groundtruth_classes']\n",
    "    boxes = data['groundtruth_boxes']\n",
    "    is_crowds = data['groundtruth_is_crowd']\n",
    "\n",
    "    # Gets original image and its size.\n",
    "    image = data['image']\n",
    "\n",
    "    image_shape = tf.shape(input=image)[0:2]\n",
    "\n",
    "    # Normalizes image with mean and std pixel values.\n",
    "    image = preprocess_ops.normalize_image(image)\n",
    "\n",
    "    # Flips image randomly during training.\n",
    "    image, boxes, _ = preprocess_ops.random_horizontal_flip(image, boxes)\n",
    "\n",
    "    # Converts boxes from normalized coordinates to pixel coordinates.\n",
    "    boxes = box_ops.denormalize_boxes(boxes, image_shape)\n",
    "\n",
    "    OUTPUT_SIZE = 4\n",
    "    MAX_LEVEL = 7\n",
    "    AUG_SCALE_MIN = 0.7\n",
    "    AUG_SCALE_MAX = 2.0\n",
    "    # Resizes and crops image.\n",
    "#    image, image_info = preprocess_ops.resize_and_crop_image(\n",
    "#        image,\n",
    "#        OUTPUT_SIZE,\n",
    "#        padded_size=preprocess_ops.compute_padded_size(OUTPUT_SIZE,\n",
    "#                                                       2**MAX_LEVEL),\n",
    "#        aug_scale_min=AUG_SCALE_MIN,\n",
    "#        aug_scale_max=AUG_SCALE_MAX)\n",
    "#    image_height, image_width, _ = image.get_shape().as_list()\n",
    "\n",
    "    # Resizes and crops boxes.\n",
    "#    image_scale = image_info[2, :]\n",
    "#    offset = image_info[3, :]\n",
    "#    boxes = preprocess_ops.resize_and_crop_boxes(boxes, image_scale,\n",
    "#                                                 image_info[1, :], offset)\n",
    "    # Filters out ground truth boxes that are all zeros.\n",
    "    indices = box_ops.get_non_empty_box_indices(boxes)\n",
    "    boxes = tf.gather(boxes, indices)\n",
    "    classes = tf.gather(classes, indices)\n",
    "\n",
    "#    # Assigns anchors.\n",
    "#    input_anchor = anchor.build_anchor_generator(\n",
    "#        min_level=self._min_level,\n",
    "#        max_level=self._max_level,\n",
    "#        num_scales=self._num_scales,\n",
    "#        aspect_ratios=self._aspect_ratios,\n",
    "#        anchor_size=self._anchor_size)\n",
    "#    anchor_boxes = input_anchor(image_size=(image_height, image_width))\n",
    "#    anchor_labeler = anchor.AnchorLabeler(self._match_threshold,\n",
    "#                                          self._unmatched_threshold)\n",
    "#    (cls_targets, box_targets, att_targets, cls_weights,\n",
    "#     box_weights) = anchor_labeler.label_anchors(\n",
    "#         anchor_boxes, boxes, tf.expand_dims(classes, axis=1), attributes)\n",
    "#\n",
    "#    # Casts input image to desired data type.\n",
    "#    image = tf.cast(image, dtype=self._dtype)\n",
    "\n",
    "#    # Packs labels for model_fn outputs.\n",
    "#    labels = {\n",
    "#        'cls_targets': cls_targets,\n",
    "#        'box_targets': box_targets,\n",
    "#        'anchor_boxes': anchor_boxes,\n",
    "#        'cls_weights': cls_weights,\n",
    "#        'box_weights': box_weights,\n",
    "#        'image_info': image_info,\n",
    "#    }\n",
    "    return image, classes, boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n"
     ]
    }
   ],
   "source": [
    "#DEBUG\n",
    "ds = ds.map(_parse_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEBUG\n",
    "for image, classes, boxes in ds.take(2):\n",
    "    #print(image.shape)\n",
    "    print(boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEBUG\n",
    "#for source_id, image, height, width, groundtruth_classes, groundtruth_is_crowd, groundtruth_area, groundtruth_boxes in ds.take(2):\n",
    "#    print(source_id, image, height, width, groundtruth_classes, groundtruth_is_crowd, groundtruth_area, groundtruth_boxes)\n",
    "for data in ds.take(200):\n",
    "    #print(data['source_id'])\n",
    "    #print(data['image'].shape)\n",
    "    print(data['width'])\n",
    "    print(data['height'])\n",
    "    print(data['groundtruth_boxes'])\n",
    "    #print(data['groundtruth_classes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PrefetchDataset shapes: ((512, 384, 384, 3), {cls_targets: OrderedDict([(3, (512, 48, 48, 9)), (4, (512, 24, 24, 9)), (5, (512, 12, 12, 9)), (6, (512, 6, 6, 9)), (7, (512, 3, 3, 9))]), box_targets: OrderedDict([(3, (512, 48, 48, 36)), (4, (512, 24, 24, 36)), (5, (512, 12, 12, 36)), (6, (512, 6, 6, 36)), (7, (512, 3, 3, 36))]), anchor_boxes: {3: (512, 48, 48, 36), 4: (512, 24, 24, 36), 5: (512, 12, 12, 36), 6: (512, 6, 6, 36), 7: (512, 3, 3, 36)}, cls_weights: (512, 27621), box_weights: (512, 27621), image_info: (512, 4, 2)}), types: (tf.bfloat16, {cls_targets: OrderedDict([(3, tf.int64), (4, tf.int64), (5, tf.int64), (6, tf.int64), (7, tf.int64)]), box_targets: OrderedDict([(3, tf.float32), (4, tf.float32), (5, tf.float32), (6, tf.float32), (7, tf.float32)]), anchor_boxes: {3: tf.float32, 4: tf.float32, 5: tf.float32, 6: tf.float32, 7: tf.float32}, cls_weights: tf.float32, box_weights: tf.float32, image_info: tf.float32})>\n",
      "<PrefetchDataset shapes: ((512, 384, 384, 3), {cls_targets: OrderedDict([(3, (512, 48, 48, 9)), (4, (512, 24, 24, 9)), (5, (512, 12, 12, 9)), (6, (512, 6, 6, 9)), (7, (512, 3, 3, 9))]), box_targets: OrderedDict([(3, (512, 48, 48, 36)), (4, (512, 24, 24, 36)), (5, (512, 12, 12, 36)), (6, (512, 6, 6, 36)), (7, (512, 3, 3, 36))]), anchor_boxes: {3: (512, 48, 48, 36), 4: (512, 24, 24, 36), 5: (512, 12, 12, 36), 6: (512, 6, 6, 36), 7: (512, 3, 3, 36)}, cls_weights: (512, 27621), box_weights: (512, 27621), image_info: (512, 4, 2), groundtruths: {source_id: (512,), height: (512,), width: (512,), num_detections: (512, 1), image_info: (512, 4, 2), boxes: (512, 100, 4), classes: (512, 100), areas: (512, 100), is_crowds: (512, 100)}}), types: (tf.bfloat16, {cls_targets: OrderedDict([(3, tf.int64), (4, tf.int64), (5, tf.int64), (6, tf.int64), (7, tf.int64)]), box_targets: OrderedDict([(3, tf.float32), (4, tf.float32), (5, tf.float32), (6, tf.float32), (7, tf.float32)]), anchor_boxes: {3: tf.float32, 4: tf.float32, 5: tf.float32, 6: tf.float32, 7: tf.float32}, cls_weights: tf.float32, box_weights: tf.float32, image_info: tf.float32, groundtruths: {source_id: tf.int64, height: tf.int64, width: tf.int64, num_detections: tf.int32, image_info: tf.float32, boxes: tf.float32, classes: tf.int64, areas: tf.float32, is_crowds: tf.int32}})>\n"
     ]
    }
   ],
   "source": [
    "#DEBUG\n",
    "train_dataset = task.build_inputs(train_data_cfg)\n",
    "print(train_dataset)\n",
    "\n",
    "valid_dataset = task.build_inputs(valid_data_cfg)\n",
    "print(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEBUG\n",
    "for image, metadata in train_dataset.take(2):\n",
    "    #print(image.shape)\n",
    "    #print(metadata.keys())\n",
    "    #print(metadata['cls_targets']['3'].shape) # all -1 ?\n",
    "    #print(metadata['box_targets']['3'][0][0][0]) # all the same ?\n",
    "    #print(metadata['cls_weights']) # all zeros ?\n",
    "    #print(metadata['image_info']) # this looks like boxes !\n",
    "    # Conclusion: ALL WRONG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG\n",
    "# model garden code copy-pasted for debugging...\n",
    "parser_config = retinanet_cfg.Parser()\n",
    "data_cfg = valid_data_cfg\n",
    "\n",
    "decoder = modelgarden.vision.beta.dataloaders.tf_example_decoder.TfExampleDecoder()\n",
    "parser = modelgarden.vision.beta.dataloaders.retinanet_input.Parser(\n",
    "    output_size=params.task.model.input_size[:2],\n",
    "    min_level=params.task.model.min_level,\n",
    "    max_level=params.task.model.max_level,\n",
    "    num_scales=params.task.model.anchor.num_scales,\n",
    "    aspect_ratios=params.task.model.anchor.aspect_ratios,\n",
    "    anchor_size=params.task.model.anchor.anchor_size,\n",
    "    dtype=data_cfg.dtype,\n",
    "    match_threshold=parser_config.match_threshold,\n",
    "    unmatched_threshold=parser_config.unmatched_threshold,\n",
    "    aug_rand_hflip=parser_config.aug_rand_hflip,\n",
    "    aug_scale_min=parser_config.aug_scale_min,\n",
    "    aug_scale_max=parser_config.aug_scale_max,\n",
    "    skip_crowd_during_training=parser_config.skip_crowd_during_training,\n",
    "    max_num_instances=parser_config.max_num_instances)\n",
    "\n",
    "reader = modelgarden.vision.beta.dataloaders.input_reader_factory.input_reader_generator(\n",
    "    data_cfg,\n",
    "    dataset_fn=modelgarden.common.dataset_fn.pick_dataset_fn(data_cfg.file_type),\n",
    "    decoder_fn=decoder.decode,\n",
    "    #parser_fn=parser.parse_fn(data_cfg.is_training))\n",
    "    parser_fn=parser.parse_fn(True))\n",
    "dataset = reader.read(input_context=None)\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG\n",
    "for image, labels in valid_dataset:\n",
    "    print(image.shape)\n",
    "    print('source_id', labels['groundtruths']['source_id'])\n",
    "    print('height', labels['groundtruths']['height'])\n",
    "    print('width', labels['groundtruths']['width'])\n",
    "    print('num_detections', labels['groundtruths']['num_detections'])\n",
    "    print('classes', labels['groundtruths']['classes'])\n",
    "    print('cls_targets', labels['cls_targets'])\n",
    "    \n",
    "    print('boxes', labels['groundtruths']['boxes'])\n",
    "    print('areas', labels['groundtruths']['areas'])\n",
    "    print('is_crowds', labels['groundtruths']['is_crowds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://ml1-demo-martin/arthropod_jobs/job1625758868\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "restoring or initializing model...\n",
      "initialized model.\n",
      "train | step:      0 | training until step 360...\n",
      "train | step:     45 | steps/sec:    0.1 | output: \n",
      "    {'box_loss': 0.009986863,\n",
      "     'cls_loss': 0.9040905,\n",
      "     'learning_rate': 0.016,\n",
      "     'model_loss': 1.4034334,\n",
      "     'total_loss': 1.4034334,\n",
      "     'training_loss': 1.4034334}\n",
      "saved checkpoint to gs://ml1-demo-martin/arthropod_jobs/job1625758868/ckpt-45.\n",
      "train | step:     90 | steps/sec:    1.6 | output: \n",
      "    {'box_loss': 0.0055036703,\n",
      "     'cls_loss': 0.63779324,\n",
      "     'learning_rate': 0.016,\n",
      "     'model_loss': 0.9129767,\n",
      "     'total_loss': 0.9129767,\n",
      "     'training_loss': 0.9129767}\n",
      "train | step:    135 | steps/sec:    2.4 | output: \n",
      "    {'box_loss': 0.004524328,\n",
      "     'cls_loss': 0.5752277,\n",
      "     'learning_rate': 0.016,\n",
      "     'model_loss': 0.801444,\n",
      "     'total_loss': 0.801444,\n",
      "     'training_loss': 0.801444}\n",
      "train | step:    180 | steps/sec:    2.3 | output: \n",
      "    {'box_loss': 0.004102429,\n",
      "     'cls_loss': 0.5308139,\n",
      "     'learning_rate': 0.016,\n",
      "     'model_loss': 0.7359352,\n",
      "     'total_loss': 0.7359352,\n",
      "     'training_loss': 0.7359352}\n",
      "train | step:    225 | steps/sec:    2.4 | output: \n",
      "    {'box_loss': 0.0038033228,\n",
      "     'cls_loss': 0.50262034,\n",
      "     'learning_rate': 0.016,\n",
      "     'model_loss': 0.6927865,\n",
      "     'total_loss': 0.6927865,\n",
      "     'training_loss': 0.6927865}\n",
      "train | step:    270 | steps/sec:    2.3 | output: \n",
      "    {'box_loss': 0.0036286132,\n",
      "     'cls_loss': 0.47913802,\n",
      "     'learning_rate': 0.016,\n",
      "     'model_loss': 0.6605688,\n",
      "     'total_loss': 0.6605688,\n",
      "     'training_loss': 0.6605688}\n",
      "train | step:    315 | steps/sec:    2.4 | output: \n",
      "    {'box_loss': 0.0034873376,\n",
      "     'cls_loss': 0.46035886,\n",
      "     'learning_rate': 0.016,\n",
      "     'model_loss': 0.6347259,\n",
      "     'total_loss': 0.6347259,\n",
      "     'training_loss': 0.6347259}\n",
      "train | step:    360 | steps/sec:    2.2 | output: \n",
      "    {'box_loss': 0.003289513,\n",
      "     'cls_loss': 0.44072634,\n",
      "     'learning_rate': 0.016,\n",
      "     'model_loss': 0.605202,\n",
      "     'total_loss': 0.605202,\n",
      "     'training_loss': 0.605202}\n",
      " eval | step:    360 | running 14 steps of evaluation...\n",
      "creating index...\n",
      "index created!\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=19.95s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=5.24s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.269\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.442\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.291\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.021\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.288\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.480\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.555\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.561\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.081\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.604\n",
      " eval | step:    360 | eval time:   90.8 sec | output: \n",
      "    {'AP': 0.2694872,\n",
      "     'AP50': 0.44155172,\n",
      "     'AP75': 0.29089224,\n",
      "     'APl': 0.2884654,\n",
      "     'APm': 0.020700648,\n",
      "     'APs': 0.0,\n",
      "     'ARl': 0.60436785,\n",
      "     'ARm': 0.08077515,\n",
      "     'ARmax1': 0.47999534,\n",
      "     'ARmax10': 0.55480117,\n",
      "     'ARmax100': 0.5609913,\n",
      "     'ARs': 0.0,\n",
      "     'box_loss': 0.0037527867,\n",
      "     'cls_loss': 0.5162054,\n",
      "     'model_loss': 0.7038447,\n",
      "     'total_loss': 0.7038447,\n",
      "     'validation_loss': 0.7038447}\n",
      "train | step:    360 | training until step 720...\n",
      "train | step:    405 | steps/sec:    0.4 | output: \n",
      "    {'box_loss': 0.0031731213,\n",
      "     'cls_loss': 0.42557743,\n",
      "     'learning_rate': 0.016,\n",
      "     'model_loss': 0.5842335,\n",
      "     'total_loss': 0.5842335,\n",
      "     'training_loss': 0.5842335}\n",
      "saved checkpoint to gs://ml1-demo-martin/arthropod_jobs/job1625758868/ckpt-405.\n",
      "train | step:    450 | steps/sec:    1.8 | output: \n",
      "    {'box_loss': 0.00312107,\n",
      "     'cls_loss': 0.4095477,\n",
      "     'learning_rate': 0.016,\n",
      "     'model_loss': 0.5656013,\n",
      "     'total_loss': 0.5656013,\n",
      "     'training_loss': 0.5656013}\n",
      "train | step:    495 | steps/sec:    2.3 | output: \n",
      "    {'box_loss': 0.00302435,\n",
      "     'cls_loss': 0.39706776,\n",
      "     'learning_rate': 0.016,\n",
      "     'model_loss': 0.5482852,\n",
      "     'total_loss': 0.5482852,\n",
      "     'training_loss': 0.5482852}\n",
      "train | step:    540 | steps/sec:    2.4 | output: \n",
      "    {'box_loss': 0.0029113882,\n",
      "     'cls_loss': 0.38970402,\n",
      "     'learning_rate': 0.016,\n",
      "     'model_loss': 0.53527355,\n",
      "     'total_loss': 0.53527355,\n",
      "     'training_loss': 0.53527355}\n",
      "train | step:    585 | steps/sec:    2.2 | output: \n",
      "    {'box_loss': 0.0028355962,\n",
      "     'cls_loss': 0.3765045,\n",
      "     'learning_rate': 0.016,\n",
      "     'model_loss': 0.5182844,\n",
      "     'total_loss': 0.5182844,\n",
      "     'training_loss': 0.5182844}\n",
      "train | step:    630 | steps/sec:    2.3 | output: \n",
      "    {'box_loss': 0.0028019883,\n",
      "     'cls_loss': 0.37118953,\n",
      "     'learning_rate': 0.016,\n",
      "     'model_loss': 0.51128894,\n",
      "     'total_loss': 0.51128894,\n",
      "     'training_loss': 0.51128894}\n",
      "train | step:    675 | steps/sec:    2.4 | output: \n",
      "    {'box_loss': 0.0026962708,\n",
      "     'cls_loss': 0.35969827,\n",
      "     'learning_rate': 0.016,\n",
      "     'model_loss': 0.49451187,\n",
      "     'total_loss': 0.49451187,\n",
      "     'training_loss': 0.49451187}\n",
      "train | step:    720 | steps/sec:    2.2 | output: \n",
      "    {'box_loss': 0.0027057163,\n",
      "     'cls_loss': 0.3520247,\n",
      "     'learning_rate': 0.008,\n",
      "     'model_loss': 0.48731053,\n",
      "     'total_loss': 0.48731053,\n",
      "     'training_loss': 0.48731053}\n",
      " eval | step:    720 | running 14 steps of evaluation...\n",
      "creating index...\n",
      "index created!\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=15.90s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=4.59s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.400\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.593\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.443\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.030\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.430\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.534\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.607\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.615\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.114\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.660\n",
      " eval | step:    720 | eval time:   50.6 sec | output: \n",
      "    {'AP': 0.3995626,\n",
      "     'AP50': 0.5928297,\n",
      "     'AP75': 0.44301173,\n",
      "     'APl': 0.42969263,\n",
      "     'APm': 0.030471954,\n",
      "     'APs': 0.0,\n",
      "     'ARl': 0.66020447,\n",
      "     'ARm': 0.114237376,\n",
      "     'ARmax1': 0.533884,\n",
      "     'ARmax10': 0.60709816,\n",
      "     'ARmax100': 0.61487764,\n",
      "     'ARs': 0.0,\n",
      "     'box_loss': 0.0031382614,\n",
      "     'cls_loss': 0.40567037,\n",
      "     'model_loss': 0.56258345,\n",
      "     'total_loss': 0.56258345,\n",
      "     'validation_loss': 0.56258345}\n",
      "train | step:    720 | training until step 1080...\n",
      "train | step:    765 | steps/sec:    0.6 | output: \n",
      "    {'box_loss': 0.0026512926,\n",
      "     'cls_loss': 0.34390268,\n",
      "     'learning_rate': 0.008,\n",
      "     'model_loss': 0.4764673,\n",
      "     'total_loss': 0.4764673,\n",
      "     'training_loss': 0.4764673}\n",
      "saved checkpoint to gs://ml1-demo-martin/arthropod_jobs/job1625758868/ckpt-765.\n",
      "train | step:    810 | steps/sec:    1.9 | output: \n",
      "    {'box_loss': 0.0025516332,\n",
      "     'cls_loss': 0.33170804,\n",
      "     'learning_rate': 0.008,\n",
      "     'model_loss': 0.45928973,\n",
      "     'total_loss': 0.45928973,\n",
      "     'training_loss': 0.45928973}\n",
      "train | step:    855 | steps/sec:    3.1 | output: \n",
      "    {'box_loss': 0.0025287406,\n",
      "     'cls_loss': 0.32887766,\n",
      "     'learning_rate': 0.008,\n",
      "     'model_loss': 0.45531476,\n",
      "     'total_loss': 0.45531476,\n",
      "     'training_loss': 0.45531476}\n",
      "train | step:    900 | steps/sec:    2.2 | output: \n",
      "    {'box_loss': 0.002494859,\n",
      "     'cls_loss': 0.3243775,\n",
      "     'learning_rate': 0.008,\n",
      "     'model_loss': 0.44912037,\n",
      "     'total_loss': 0.44912037,\n",
      "     'training_loss': 0.44912037}\n",
      "train | step:    945 | steps/sec:    2.3 | output: \n",
      "    {'box_loss': 0.0025396033,\n",
      "     'cls_loss': 0.32428423,\n",
      "     'learning_rate': 0.008,\n",
      "     'model_loss': 0.4512644,\n",
      "     'total_loss': 0.4512644,\n",
      "     'training_loss': 0.4512644}\n",
      "train | step:    990 | steps/sec:    2.3 | output: \n",
      "    {'box_loss': 0.0025021324,\n",
      "     'cls_loss': 0.31988975,\n",
      "     'learning_rate': 0.008,\n",
      "     'model_loss': 0.4449964,\n",
      "     'total_loss': 0.4449964,\n",
      "     'training_loss': 0.4449964}\n",
      "train | step:   1035 | steps/sec:    2.4 | output: \n",
      "    {'box_loss': 0.0025041427,\n",
      "     'cls_loss': 0.31857705,\n",
      "     'learning_rate': 0.008,\n",
      "     'model_loss': 0.4437841,\n",
      "     'total_loss': 0.4437841,\n",
      "     'training_loss': 0.4437841}\n",
      "train | step:   1080 | steps/sec:    2.1 | output: \n",
      "    {'box_loss': 0.0024264199,\n",
      "     'cls_loss': 0.3087332,\n",
      "     'learning_rate': 0.008,\n",
      "     'model_loss': 0.43005425,\n",
      "     'total_loss': 0.43005425,\n",
      "     'training_loss': 0.43005425}\n",
      " eval | step:   1080 | running 14 steps of evaluation...\n",
      "creating index...\n",
      "index created!\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=19.22s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=4.48s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.429\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.625\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.469\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.031\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.461\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.547\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.618\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.626\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.128\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.671\n",
      " eval | step:   1080 | eval time:   57.1 sec | output: \n",
      "    {'AP': 0.42856407,\n",
      "     'AP50': 0.6246452,\n",
      "     'AP75': 0.46884012,\n",
      "     'APl': 0.46093306,\n",
      "     'APm': 0.03062287,\n",
      "     'APs': 0.0,\n",
      "     'ARl': 0.67129236,\n",
      "     'ARm': 0.12766738,\n",
      "     'ARmax1': 0.5472921,\n",
      "     'ARmax10': 0.61843073,\n",
      "     'ARmax100': 0.62626296,\n",
      "     'ARs': 0.0,\n",
      "     'box_loss': 0.0030017456,\n",
      "     'cls_loss': 0.38695148,\n",
      "     'model_loss': 0.53703874,\n",
      "     'total_loss': 0.53703874,\n",
      "     'validation_loss': 0.53703874}\n",
      "train | step:   1080 | training until step 1440...\n",
      "train | step:   1125 | steps/sec:    0.6 | output: \n",
      "    {'box_loss': 0.0024494242,\n",
      "     'cls_loss': 0.30961457,\n",
      "     'learning_rate': 0.008,\n",
      "     'model_loss': 0.4320858,\n",
      "     'total_loss': 0.4320858,\n",
      "     'training_loss': 0.4320858}\n",
      "saved checkpoint to gs://ml1-demo-martin/arthropod_jobs/job1625758868/ckpt-1125.\n",
      "train | step:   1170 | steps/sec:    1.9 | output: \n",
      "    {'box_loss': 0.002409944,\n",
      "     'cls_loss': 0.30645123,\n",
      "     'learning_rate': 0.008,\n",
      "     'model_loss': 0.42694852,\n",
      "     'total_loss': 0.42694852,\n",
      "     'training_loss': 0.42694852}\n",
      "train | step:   1215 | steps/sec:    3.4 | output: \n",
      "    {'box_loss': 0.002385736,\n",
      "     'cls_loss': 0.30119908,\n",
      "     'learning_rate': 0.008,\n",
      "     'model_loss': 0.42048585,\n",
      "     'total_loss': 0.42048585,\n",
      "     'training_loss': 0.42048585}\n",
      "train | step:   1260 | steps/sec:    3.0 | output: \n",
      "    {'box_loss': 0.0023723526,\n",
      "     'cls_loss': 0.29741606,\n",
      "     'learning_rate': 0.008,\n",
      "     'model_loss': 0.4160336,\n",
      "     'total_loss': 0.4160336,\n",
      "     'training_loss': 0.4160336}\n",
      "train | step:   1305 | steps/sec:    2.4 | output: \n",
      "    {'box_loss': 0.0023969468,\n",
      "     'cls_loss': 0.29677504,\n",
      "     'learning_rate': 0.008,\n",
      "     'model_loss': 0.41662237,\n",
      "     'total_loss': 0.41662237,\n",
      "     'training_loss': 0.41662237}\n",
      "train | step:   1350 | steps/sec:    2.3 | output: \n",
      "    {'box_loss': 0.0023624804,\n",
      "     'cls_loss': 0.29336676,\n",
      "     'learning_rate': 0.008,\n",
      "     'model_loss': 0.4114908,\n",
      "     'total_loss': 0.4114908,\n",
      "     'training_loss': 0.4114908}\n",
      "train | step:   1395 | steps/sec:    2.3 | output: \n",
      "    {'box_loss': 0.0023045405,\n",
      "     'cls_loss': 0.28876102,\n",
      "     'learning_rate': 0.004,\n",
      "     'model_loss': 0.403988,\n",
      "     'total_loss': 0.403988,\n",
      "     'training_loss': 0.403988}\n",
      "train | step:   1440 | steps/sec:    2.4 | output: \n",
      "    {'box_loss': 0.0022957602,\n",
      "     'cls_loss': 0.28585145,\n",
      "     'learning_rate': 0.004,\n",
      "     'model_loss': 0.4006394,\n",
      "     'total_loss': 0.4006394,\n",
      "     'training_loss': 0.4006394}\n",
      " eval | step:   1440 | running 14 steps of evaluation...\n",
      "creating index...\n",
      "index created!\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=15.56s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=4.39s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.456\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.657\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.500\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.033\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.490\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.557\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.627\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.635\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.126\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.680\n",
      " eval | step:   1440 | eval time:   51.4 sec | output: \n",
      "    {'AP': 0.45579487,\n",
      "     'AP50': 0.65662044,\n",
      "     'AP75': 0.4999961,\n",
      "     'APl': 0.49042276,\n",
      "     'APm': 0.033033073,\n",
      "     'APs': 0.0,\n",
      "     'ARl': 0.68045557,\n",
      "     'ARm': 0.12565431,\n",
      "     'ARmax1': 0.5569207,\n",
      "     'ARmax10': 0.62732387,\n",
      "     'ARmax100': 0.6349464,\n",
      "     'ARs': 0.0,\n",
      "     'box_loss': 0.0029077646,\n",
      "     'cls_loss': 0.3695106,\n",
      "     'model_loss': 0.5148988,\n",
      "     'total_loss': 0.5148988,\n",
      "     'validation_loss': 0.5148988}\n",
      "train | step:   1440 | training until step 1800...\n",
      "train | step:   1485 | steps/sec:    0.6 | output: \n",
      "    {'box_loss': 0.0022494814,\n",
      "     'cls_loss': 0.27941972,\n",
      "     'learning_rate': 0.004,\n",
      "     'model_loss': 0.3918937,\n",
      "     'total_loss': 0.3918937,\n",
      "     'training_loss': 0.3918937}\n",
      "saved checkpoint to gs://ml1-demo-martin/arthropod_jobs/job1625758868/ckpt-1485.\n",
      "train | step:   1530 | steps/sec:    2.0 | output: \n",
      "    {'box_loss': 0.002245737,\n",
      "     'cls_loss': 0.27767816,\n",
      "     'learning_rate': 0.004,\n",
      "     'model_loss': 0.38996497,\n",
      "     'total_loss': 0.38996497,\n",
      "     'training_loss': 0.38996497}\n",
      "train | step:   1575 | steps/sec:    3.4 | output: \n",
      "    {'box_loss': 0.0022607157,\n",
      "     'cls_loss': 0.27725527,\n",
      "     'learning_rate': 0.004,\n",
      "     'model_loss': 0.3902911,\n",
      "     'total_loss': 0.3902911,\n",
      "     'training_loss': 0.3902911}\n",
      "train | step:   1620 | steps/sec:    3.2 | output: \n",
      "    {'box_loss': 0.0022485575,\n",
      "     'cls_loss': 0.27738252,\n",
      "     'learning_rate': 0.004,\n",
      "     'model_loss': 0.38981038,\n",
      "     'total_loss': 0.38981038,\n",
      "     'training_loss': 0.38981038}\n",
      "train | step:   1665 | steps/sec:    3.4 | output: \n",
      "    {'box_loss': 0.002226727,\n",
      "     'cls_loss': 0.27373648,\n",
      "     'learning_rate': 0.004,\n",
      "     'model_loss': 0.38507283,\n",
      "     'total_loss': 0.38507283,\n",
      "     'training_loss': 0.38507283}\n",
      "train | step:   1710 | steps/sec:    2.6 | output: \n",
      "    {'box_loss': 0.002256781,\n",
      "     'cls_loss': 0.2738679,\n",
      "     'learning_rate': 0.004,\n",
      "     'model_loss': 0.386707,\n",
      "     'total_loss': 0.386707,\n",
      "     'training_loss': 0.386707}\n",
      "train | step:   1755 | steps/sec:    2.3 | output: \n",
      "    {'box_loss': 0.002236362,\n",
      "     'cls_loss': 0.27384967,\n",
      "     'learning_rate': 0.004,\n",
      "     'model_loss': 0.38566777,\n",
      "     'total_loss': 0.38566777,\n",
      "     'training_loss': 0.38566777}\n",
      "train | step:   1800 | steps/sec:    2.2 | output: \n",
      "    {'box_loss': 0.0021747733,\n",
      "     'cls_loss': 0.2701319,\n",
      "     'learning_rate': 0.004,\n",
      "     'model_loss': 0.3788706,\n",
      "     'total_loss': 0.3788706,\n",
      "     'training_loss': 0.3788706}\n",
      " eval | step:   1800 | running 14 steps of evaluation...\n",
      "creating index...\n",
      "index created!\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=15.83s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=8.01s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.467\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.669\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.516\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.038\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.503\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.559\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.631\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.640\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.135\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.685\n",
      " eval | step:   1800 | eval time:   59.1 sec | output: \n",
      "    {'AP': 0.46747917,\n",
      "     'AP50': 0.6689523,\n",
      "     'AP75': 0.51613903,\n",
      "     'APl': 0.50282294,\n",
      "     'APm': 0.03787436,\n",
      "     'APs': 0.0,\n",
      "     'ARl': 0.6849684,\n",
      "     'ARm': 0.13471358,\n",
      "     'ARmax1': 0.5594951,\n",
      "     'ARmax10': 0.63055575,\n",
      "     'ARmax100': 0.6395154,\n",
      "     'ARs': 0.0,\n",
      "     'box_loss': 0.002874747,\n",
      "     'cls_loss': 0.35876042,\n",
      "     'model_loss': 0.5024978,\n",
      "     'total_loss': 0.5024978,\n",
      "     'validation_loss': 0.5024978}\n",
      "train | step:   1800 | training until step 2160...\n",
      "train | step:   1845 | steps/sec:    0.5 | output: \n",
      "    {'box_loss': 0.0022328072,\n",
      "     'cls_loss': 0.27062508,\n",
      "     'learning_rate': 0.004,\n",
      "     'model_loss': 0.38226548,\n",
      "     'total_loss': 0.38226548,\n",
      "     'training_loss': 0.38226548}\n",
      "saved checkpoint to gs://ml1-demo-martin/arthropod_jobs/job1625758868/ckpt-1845.\n",
      "train | step:   1890 | steps/sec:    1.8 | output: \n",
      "    {'box_loss': 0.0022039763,\n",
      "     'cls_loss': 0.26949754,\n",
      "     'learning_rate': 0.004,\n",
      "     'model_loss': 0.3796963,\n",
      "     'total_loss': 0.3796963,\n",
      "     'training_loss': 0.3796963}\n",
      "train | step:   1935 | steps/sec:    3.0 | output: \n",
      "    {'box_loss': 0.002226146,\n",
      "     'cls_loss': 0.26740542,\n",
      "     'learning_rate': 0.004,\n",
      "     'model_loss': 0.37871268,\n",
      "     'total_loss': 0.37871268,\n",
      "     'training_loss': 0.37871268}\n",
      "train | step:   1980 | steps/sec:    3.1 | output: \n",
      "    {'box_loss': 0.0021411437,\n",
      "     'cls_loss': 0.26442912,\n",
      "     'learning_rate': 0.004,\n",
      "     'model_loss': 0.37148628,\n",
      "     'total_loss': 0.37148628,\n",
      "     'training_loss': 0.37148628}\n",
      "train | step:   2025 | steps/sec:    2.9 | output: \n",
      "    {'box_loss': 0.0021790918,\n",
      "     'cls_loss': 0.26519418,\n",
      "     'learning_rate': 0.004,\n",
      "     'model_loss': 0.37414882,\n",
      "     'total_loss': 0.37414882,\n",
      "     'training_loss': 0.37414882}\n",
      "train | step:   2070 | steps/sec:    1.3 | output: \n",
      "    {'box_loss': 0.0021880583,\n",
      "     'cls_loss': 0.26407868,\n",
      "     'learning_rate': 0.002,\n",
      "     'model_loss': 0.37348166,\n",
      "     'total_loss': 0.37348166,\n",
      "     'training_loss': 0.37348166}\n",
      "train | step:   2115 | steps/sec:    1.4 | output: \n",
      "    {'box_loss': 0.0021540131,\n",
      "     'cls_loss': 0.2589518,\n",
      "     'learning_rate': 0.002,\n",
      "     'model_loss': 0.36665243,\n",
      "     'total_loss': 0.36665243,\n",
      "     'training_loss': 0.36665243}\n",
      "train | step:   2160 | steps/sec:    1.4 | output: \n",
      "    {'box_loss': 0.0021219014,\n",
      "     'cls_loss': 0.25640535,\n",
      "     'learning_rate': 0.002,\n",
      "     'model_loss': 0.3625003,\n",
      "     'total_loss': 0.3625003,\n",
      "     'training_loss': 0.3625003}\n",
      " eval | step:   2160 | running 14 steps of evaluation...\n",
      "creating index...\n",
      "index created!\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=15.97s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=4.44s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.480\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.684\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.524\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.036\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.516\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.565\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.636\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.645\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.128\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.691\n",
      " eval | step:   2160 | eval time:   60.5 sec | output: \n",
      "    {'AP': 0.479916,\n",
      "     'AP50': 0.68364966,\n",
      "     'AP75': 0.5237834,\n",
      "     'APl': 0.5162052,\n",
      "     'APm': 0.0363642,\n",
      "     'APs': 0.0,\n",
      "     'ARl': 0.6907806,\n",
      "     'ARm': 0.12751074,\n",
      "     'ARmax1': 0.56472933,\n",
      "     'ARmax10': 0.63622934,\n",
      "     'ARmax100': 0.64486974,\n",
      "     'ARs': 0.0,\n",
      "     'box_loss': 0.0028154124,\n",
      "     'cls_loss': 0.35588053,\n",
      "     'model_loss': 0.49665117,\n",
      "     'total_loss': 0.49665117,\n",
      "     'validation_loss': 0.49665117}\n",
      "train | step:   2160 | training until step 2520...\n",
      "train | step:   2205 | steps/sec:    0.5 | output: \n",
      "    {'box_loss': 0.0021436163,\n",
      "     'cls_loss': 0.2553428,\n",
      "     'learning_rate': 0.002,\n",
      "     'model_loss': 0.36252365,\n",
      "     'total_loss': 0.36252365,\n",
      "     'training_loss': 0.36252365}\n",
      "saved checkpoint to gs://ml1-demo-martin/arthropod_jobs/job1625758868/ckpt-2205.\n",
      "train | step:   2250 | steps/sec:    1.8 | output: \n",
      "    {'box_loss': 0.0021508897,\n",
      "     'cls_loss': 0.2596987,\n",
      "     'learning_rate': 0.002,\n",
      "     'model_loss': 0.36724314,\n",
      "     'total_loss': 0.36724314,\n",
      "     'training_loss': 0.36724314}\n",
      "train | step:   2295 | steps/sec:    3.1 | output: \n",
      "    {'box_loss': 0.0021426226,\n",
      "     'cls_loss': 0.2559128,\n",
      "     'learning_rate': 0.002,\n",
      "     'model_loss': 0.36304402,\n",
      "     'total_loss': 0.36304402,\n",
      "     'training_loss': 0.36304402}\n",
      "train | step:   2340 | steps/sec:    3.0 | output: \n",
      "    {'box_loss': 0.002127244,\n",
      "     'cls_loss': 0.25763676,\n",
      "     'learning_rate': 0.002,\n",
      "     'model_loss': 0.36399895,\n",
      "     'total_loss': 0.36399895,\n",
      "     'training_loss': 0.36399895}\n",
      "train | step:   2385 | steps/sec:    1.5 | output: \n",
      "    {'box_loss': 0.0021653413,\n",
      "     'cls_loss': 0.25820547,\n",
      "     'learning_rate': 0.002,\n",
      "     'model_loss': 0.36647257,\n",
      "     'total_loss': 0.36647257,\n",
      "     'training_loss': 0.36647257}\n",
      "train | step:   2430 | steps/sec:    1.3 | output: \n",
      "    {'box_loss': 0.0020721632,\n",
      "     'cls_loss': 0.25210467,\n",
      "     'learning_rate': 0.002,\n",
      "     'model_loss': 0.3557128,\n",
      "     'total_loss': 0.3557128,\n",
      "     'training_loss': 0.3557128}\n",
      "train | step:   2475 | steps/sec:    1.4 | output: \n",
      "    {'box_loss': 0.0021025958,\n",
      "     'cls_loss': 0.2519383,\n",
      "     'learning_rate': 0.002,\n",
      "     'model_loss': 0.35706812,\n",
      "     'total_loss': 0.35706812,\n",
      "     'training_loss': 0.35706812}\n",
      "train | step:   2520 | steps/sec:    1.3 | output: \n",
      "    {'box_loss': 0.002098589,\n",
      "     'cls_loss': 0.2539477,\n",
      "     'learning_rate': 0.002,\n",
      "     'model_loss': 0.35887715,\n",
      "     'total_loss': 0.35887715,\n",
      "     'training_loss': 0.35887715}\n",
      " eval | step:   2520 | running 14 steps of evaluation...\n",
      "creating index...\n",
      "index created!\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=15.50s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=4.33s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.478\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.679\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.523\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.035\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.514\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.564\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.633\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.641\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.121\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.687\n",
      " eval | step:   2520 | eval time:   61.1 sec | output: \n",
      "    {'AP': 0.4777084,\n",
      "     'AP50': 0.67897016,\n",
      "     'AP75': 0.5232064,\n",
      "     'APl': 0.5140515,\n",
      "     'APm': 0.035308383,\n",
      "     'APs': 0.0,\n",
      "     'ARl': 0.6869147,\n",
      "     'ARm': 0.1213686,\n",
      "     'ARmax1': 0.5636189,\n",
      "     'ARmax10': 0.63268495,\n",
      "     'ARmax100': 0.64083034,\n",
      "     'ARs': 0.0,\n",
      "     'box_loss': 0.0028093008,\n",
      "     'cls_loss': 0.35826018,\n",
      "     'model_loss': 0.49872527,\n",
      "     'total_loss': 0.49872527,\n",
      "     'validation_loss': 0.49872527}\n",
      "train | step:   2520 | training until step 2880...\n",
      "train | step:   2565 | steps/sec:    0.5 | output: \n",
      "    {'box_loss': 0.0020930457,\n",
      "     'cls_loss': 0.25194407,\n",
      "     'learning_rate': 0.002,\n",
      "     'model_loss': 0.35659638,\n",
      "     'total_loss': 0.35659638,\n",
      "     'training_loss': 0.35659638}\n",
      "saved checkpoint to gs://ml1-demo-martin/arthropod_jobs/job1625758868/ckpt-2565.\n",
      "train | step:   2610 | steps/sec:    1.8 | output: \n",
      "    {'box_loss': 0.002118862,\n",
      "     'cls_loss': 0.25232756,\n",
      "     'learning_rate': 0.002,\n",
      "     'model_loss': 0.35827065,\n",
      "     'total_loss': 0.35827065,\n",
      "     'training_loss': 0.35827065}\n",
      "train | step:   2655 | steps/sec:    3.1 | output: \n",
      "    {'box_loss': 0.0020848552,\n",
      "     'cls_loss': 0.25030428,\n",
      "     'learning_rate': 0.002,\n",
      "     'model_loss': 0.35454702,\n",
      "     'total_loss': 0.35454702,\n",
      "     'training_loss': 0.35454702}\n",
      "train | step:   2700 | steps/sec:    2.9 | output: \n",
      "    {'box_loss': 0.0020668819,\n",
      "     'cls_loss': 0.25170496,\n",
      "     'learning_rate': 0.002,\n",
      "     'model_loss': 0.355049,\n",
      "     'total_loss': 0.355049,\n",
      "     'training_loss': 0.355049}\n",
      "train | step:   2745 | steps/sec:    1.4 | output: \n",
      "    {'box_loss': 0.0021005976,\n",
      "     'cls_loss': 0.25087464,\n",
      "     'learning_rate': 0.001,\n",
      "     'model_loss': 0.35590452,\n",
      "     'total_loss': 0.35590452,\n",
      "     'training_loss': 0.35590452}\n",
      "train | step:   2790 | steps/sec:    1.3 | output: \n",
      "    {'box_loss': 0.0020828547,\n",
      "     'cls_loss': 0.2493642,\n",
      "     'learning_rate': 0.001,\n",
      "     'model_loss': 0.35350695,\n",
      "     'total_loss': 0.35350695,\n",
      "     'training_loss': 0.35350695}\n",
      "train | step:   2835 | steps/sec:    1.4 | output: \n",
      "    {'box_loss': 0.0020326786,\n",
      "     'cls_loss': 0.24600635,\n",
      "     'learning_rate': 0.001,\n",
      "     'model_loss': 0.3476403,\n",
      "     'total_loss': 0.3476403,\n",
      "     'training_loss': 0.3476403}\n",
      "train | step:   2880 | steps/sec:    1.3 | output: \n",
      "    {'box_loss': 0.002095527,\n",
      "     'cls_loss': 0.2466666,\n",
      "     'learning_rate': 0.001,\n",
      "     'model_loss': 0.35144293,\n",
      "     'total_loss': 0.35144293,\n",
      "     'training_loss': 0.35144293}\n",
      " eval | step:   2880 | running 14 steps of evaluation...\n",
      "creating index...\n",
      "index created!\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=15.43s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=4.27s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.483\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.688\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.528\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.039\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.519\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.564\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.635\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.644\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.147\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.688\n",
      " eval | step:   2880 | eval time:   61.1 sec | output: \n",
      "    {'AP': 0.48257026,\n",
      "     'AP50': 0.6878272,\n",
      "     'AP75': 0.52765244,\n",
      "     'APl': 0.51912713,\n",
      "     'APm': 0.038953893,\n",
      "     'APs': 0.0,\n",
      "     'ARl': 0.6884022,\n",
      "     'ARm': 0.14691819,\n",
      "     'ARmax1': 0.5637708,\n",
      "     'ARmax10': 0.6345552,\n",
      "     'ARmax100': 0.6439916,\n",
      "     'ARs': 0.0,\n",
      "     'box_loss': 0.0028012965,\n",
      "     'cls_loss': 0.35491017,\n",
      "     'model_loss': 0.494975,\n",
      "     'total_loss': 0.494975,\n",
      "     'validation_loss': 0.494975}\n",
      "train | step:   2880 | training until step 3240...\n",
      "train | step:   2925 | steps/sec:    0.5 | output: \n",
      "    {'box_loss': 0.0020824524,\n",
      "     'cls_loss': 0.24605107,\n",
      "     'learning_rate': 0.001,\n",
      "     'model_loss': 0.35017368,\n",
      "     'total_loss': 0.35017368,\n",
      "     'training_loss': 0.35017368}\n",
      "saved checkpoint to gs://ml1-demo-martin/arthropod_jobs/job1625758868/ckpt-2925.\n",
      "train | step:   2970 | steps/sec:    1.8 | output: \n",
      "    {'box_loss': 0.0020345687,\n",
      "     'cls_loss': 0.2436413,\n",
      "     'learning_rate': 0.001,\n",
      "     'model_loss': 0.34536967,\n",
      "     'total_loss': 0.34536967,\n",
      "     'training_loss': 0.34536967}\n",
      "train | step:   3015 | steps/sec:    3.1 | output: \n",
      "    {'box_loss': 0.0020487744,\n",
      "     'cls_loss': 0.24570775,\n",
      "     'learning_rate': 0.001,\n",
      "     'model_loss': 0.34814647,\n",
      "     'total_loss': 0.34814647,\n",
      "     'training_loss': 0.34814647}\n",
      "train | step:   3060 | steps/sec:    2.9 | output: \n",
      "    {'box_loss': 0.0020999396,\n",
      "     'cls_loss': 0.2468491,\n",
      "     'learning_rate': 0.001,\n",
      "     'model_loss': 0.351846,\n",
      "     'total_loss': 0.351846,\n",
      "     'training_loss': 0.351846}\n",
      "train | step:   3105 | steps/sec:    1.8 | output: \n",
      "    {'box_loss': 0.002058673,\n",
      "     'cls_loss': 0.24472809,\n",
      "     'learning_rate': 0.001,\n",
      "     'model_loss': 0.3476617,\n",
      "     'total_loss': 0.3476617,\n",
      "     'training_loss': 0.3476617}\n",
      "train | step:   3150 | steps/sec:    1.3 | output: \n",
      "    {'box_loss': 0.0020760375,\n",
      "     'cls_loss': 0.24480559,\n",
      "     'learning_rate': 0.001,\n",
      "     'model_loss': 0.34860748,\n",
      "     'total_loss': 0.34860748,\n",
      "     'training_loss': 0.34860748}\n",
      "train | step:   3195 | steps/sec:    1.4 | output: \n",
      "    {'box_loss': 0.002046574,\n",
      "     'cls_loss': 0.2450529,\n",
      "     'learning_rate': 0.001,\n",
      "     'model_loss': 0.34738162,\n",
      "     'total_loss': 0.34738162,\n",
      "     'training_loss': 0.34738162}\n",
      "train | step:   3240 | steps/sec:    1.3 | output: \n",
      "    {'box_loss': 0.0020687918,\n",
      "     'cls_loss': 0.24537238,\n",
      "     'learning_rate': 0.001,\n",
      "     'model_loss': 0.34881195,\n",
      "     'total_loss': 0.34881195,\n",
      "     'training_loss': 0.34881195}\n",
      " eval | step:   3240 | running 14 steps of evaluation...\n",
      "creating index...\n",
      "index created!\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=18.68s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=4.28s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.485\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.689\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.531\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.041\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.521\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.566\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.637\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.645\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.135\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.691\n",
      " eval | step:   3240 | eval time:   63.9 sec | output: \n",
      "    {'AP': 0.48459944,\n",
      "     'AP50': 0.6891592,\n",
      "     'AP75': 0.531033,\n",
      "     'APl': 0.52123916,\n",
      "     'APm': 0.041021094,\n",
      "     'APs': 0.0,\n",
      "     'ARl': 0.69054765,\n",
      "     'ARm': 0.13494898,\n",
      "     'ARmax1': 0.5663472,\n",
      "     'ARmax10': 0.6366415,\n",
      "     'ARmax100': 0.6452313,\n",
      "     'ARs': 0.0,\n",
      "     'box_loss': 0.0027820324,\n",
      "     'cls_loss': 0.3541915,\n",
      "     'model_loss': 0.4932932,\n",
      "     'total_loss': 0.4932932,\n",
      "     'validation_loss': 0.4932932}\n",
      "train | step:   3240 | training until step 3600...\n",
      "train | step:   3285 | steps/sec:    0.5 | output: \n",
      "    {'box_loss': 0.0020246168,\n",
      "     'cls_loss': 0.24276102,\n",
      "     'learning_rate': 0.001,\n",
      "     'model_loss': 0.34399182,\n",
      "     'total_loss': 0.34399182,\n",
      "     'training_loss': 0.34399182}\n",
      "saved checkpoint to gs://ml1-demo-martin/arthropod_jobs/job1625758868/ckpt-3285.\n",
      "train | step:   3330 | steps/sec:    1.7 | output: \n",
      "    {'box_loss': 0.0020511749,\n",
      "     'cls_loss': 0.24307704,\n",
      "     'learning_rate': 0.001,\n",
      "     'model_loss': 0.34563577,\n",
      "     'total_loss': 0.34563577,\n",
      "     'training_loss': 0.34563577}\n",
      "train | step:   3375 | steps/sec:    3.1 | output: \n",
      "    {'box_loss': 0.002051673,\n",
      "     'cls_loss': 0.24505039,\n",
      "     'learning_rate': 0.001,\n",
      "     'model_loss': 0.347634,\n",
      "     'total_loss': 0.347634,\n",
      "     'training_loss': 0.347634}\n",
      "train | step:   3420 | steps/sec:    2.9 | output: \n",
      "    {'box_loss': 0.0020580436,\n",
      "     'cls_loss': 0.24282108,\n",
      "     'learning_rate': 0.0005,\n",
      "     'model_loss': 0.34572327,\n",
      "     'total_loss': 0.34572327,\n",
      "     'training_loss': 0.34572327}\n",
      "train | step:   3465 | steps/sec:    1.6 | output: \n",
      "    {'box_loss': 0.0020621794,\n",
      "     'cls_loss': 0.24441192,\n",
      "     'learning_rate': 0.0005,\n",
      "     'model_loss': 0.34752086,\n",
      "     'total_loss': 0.34752086,\n",
      "     'training_loss': 0.34752086}\n",
      "train | step:   3510 | steps/sec:    1.3 | output: \n",
      "    {'box_loss': 0.002061785,\n",
      "     'cls_loss': 0.24354702,\n",
      "     'learning_rate': 0.0005,\n",
      "     'model_loss': 0.3466363,\n",
      "     'total_loss': 0.3466363,\n",
      "     'training_loss': 0.3466363}\n",
      "train | step:   3555 | steps/sec:    1.3 | output: \n",
      "    {'box_loss': 0.002036447,\n",
      "     'cls_loss': 0.24052048,\n",
      "     'learning_rate': 0.0005,\n",
      "     'model_loss': 0.34234285,\n",
      "     'total_loss': 0.34234285,\n",
      "     'training_loss': 0.34234285}\n",
      "train | step:   3600 | steps/sec:    1.3 | output: \n",
      "    {'box_loss': 0.0020582546,\n",
      "     'cls_loss': 0.2415939,\n",
      "     'learning_rate': 0.0005,\n",
      "     'model_loss': 0.34450665,\n",
      "     'total_loss': 0.34450665,\n",
      "     'training_loss': 0.34450665}\n",
      " eval | step:   3600 | running 14 steps of evaluation...\n",
      "creating index...\n",
      "index created!\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=15.43s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=4.23s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.487\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.689\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.532\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.040\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.523\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.567\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.638\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.647\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.146\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.692\n",
      " eval | step:   3600 | eval time:   57.5 sec | output: \n",
      "    {'AP': 0.48650515,\n",
      "     'AP50': 0.6892873,\n",
      "     'AP75': 0.5324517,\n",
      "     'APl': 0.5234034,\n",
      "     'APm': 0.039541427,\n",
      "     'APs': 0.0,\n",
      "     'ARl': 0.69230443,\n",
      "     'ARm': 0.14600429,\n",
      "     'ARmax1': 0.56712604,\n",
      "     'ARmax10': 0.6379348,\n",
      "     'ARmax100': 0.6473965,\n",
      "     'ARs': 0.0,\n",
      "     'box_loss': 0.0027743524,\n",
      "     'cls_loss': 0.35303423,\n",
      "     'model_loss': 0.49175185,\n",
      "     'total_loss': 0.49175185,\n",
      "     'validation_loss': 0.49175185}\n",
      "saved checkpoint to gs://ml1-demo-martin/arthropod_jobs/job1625758868/ckpt-3600.\n"
     ]
    }
   ],
   "source": [
    "print(MODEL_DIR)\n",
    "model,_ = train_lib.run_experiment(\n",
    "    distribution_strategy=strategy,\n",
    "    task=task,\n",
    "    mode=\"train_and_eval\", # 'train', 'eval', 'train_and_eval' or 'continuous_eval'\n",
    "    params=params,\n",
    "    model_dir=MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <official.vision.beta.modeling.retinanet_model.RetinaNetModel object at 0x7fbcd7e2e5d0>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <official.vision.beta.modeling.retinanet_model.RetinaNetModel object at 0x7fbcd7e2e5d0>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <official.vision.beta.modeling.layers.detection_generator.MultilevelDetectionGenerator object at 0x7fbcae95d090>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <official.vision.beta.modeling.layers.detection_generator.MultilevelDetectionGenerator object at 0x7fbcae95d090>, because it is not built.\n",
      "WARNING:absl:Found untraced functions such as inference_from_image_bytes, inference_from_tf_example, retina_net_head_2_layer_call_fn, retina_net_head_2_layer_call_and_return_conditional_losses, scores_layer_call_fn while saving (showing 5 of 507). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:FOR KERAS USERS: The object that you are saving contains one or more Keras models or layers. If you are loading the SavedModel with `tf.keras.models.load_model`, continue reading (otherwise, you may ignore the following instructions). Please change your code to save with `tf.keras.models.save_model` or `model.save`, and confirm that the file \"keras.metadata\" exists in the export directory. In the future, Keras will only load the SavedModels that have this file. In other words, `tf.saved_model.save` will no longer write SavedModels that can be recovered as Keras models (this will apply in TF 2.5).\n",
      "\n",
      "FOR DEVS: If you are overwriting _tracking_metadata in your class, this property has been used to save metadata in the SavedModel. The metadta field will be deprecated soon, so please move the metadata to a different file.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:FOR KERAS USERS: The object that you are saving contains one or more Keras models or layers. If you are loading the SavedModel with `tf.keras.models.load_model`, continue reading (otherwise, you may ignore the following instructions). Please change your code to save with `tf.keras.models.save_model` or `model.save`, and confirm that the file \"keras.metadata\" exists in the export directory. In the future, Keras will only load the SavedModels that have this file. In other words, `tf.saved_model.save` will no longer write SavedModels that can be recovered as Keras models (this will apply in TF 2.5).\n",
      "\n",
      "FOR DEVS: If you are overwriting _tracking_metadata in your class, this property has been used to save metadata in the SavedModel. The metadta field will be deprecated soon, so please move the metadata to a different file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: gs://ml1-demo-martin/arthropod_jobs/job1625758868/saved_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: gs://ml1-demo-martin/arthropod_jobs/job1625758868/saved_model/assets\n"
     ]
    }
   ],
   "source": [
    "export_saved_model_lib.export_inference_graph(\n",
    "      input_type='image_tensor',\n",
    "      batch_size=4,\n",
    "      input_image_size=OUTPUT_SIZE,\n",
    "      params=params,\n",
    "      checkpoint_path=MODEL_DIR,\n",
    "      export_dir=MODEL_DIR,\n",
    "      export_checkpoint_subdir='saved_chkpt',\n",
    "      export_saved_model_subdir='saved_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## License\n",
    "Copyright 2021 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-cpu.2-5.m73",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-cpu.2-5:m73"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
